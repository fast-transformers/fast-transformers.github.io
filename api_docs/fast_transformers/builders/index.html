<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.6.dev1+g65b31fd" />
<title>fast_transformers.builders API documentation</title>
<meta name="description" content="This module implements builders that simplify building complex transformer
architectures with different attention mechanisms â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fast_transformers.builders</code></h1>
</header>
<section id="section-intro">
<p>This module implements builders that simplify building complex transformer
architectures with different attention mechanisms.</p>
<p>The main idea is to facilitate the construction of various attention layers and
transformer encoder layers and simplify their assembly into one transformer
module. It also allows for flexibility in the scripts as many builder
parameters can correspond 1-1 with command line arguments.</p>
<p>Example usage:</p>
<pre><code>builder = TransformerEncoderBuilder()
builder.n_layers = 12
builder.n_heads = 8
builder.feed_forward_dimensions = 1024
builder.query_dimensions = 64
builder.value_dimensions = 64
builder.dropout = 0.1
builder.attention_dropout = 0.1
builder.attention_type = "linear"
transformer = builder.get()
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#
# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
# Written by Angelos Katharopoulos &lt;angelos.katharopoulos@idiap.ch&gt;,
# Apoorv Vyas &lt;avyas@idiap.ch&gt;
#

&#34;&#34;&#34;This module implements builders that simplify building complex transformer
architectures with different attention mechanisms.

The main idea is to facilitate the construction of various attention layers and
transformer encoder layers and simplify their assembly into one transformer
module. It also allows for flexibility in the scripts as many builder
parameters can correspond 1-1 with command line arguments.

Example usage:

    builder = TransformerEncoderBuilder()
    builder.n_layers = 12
    builder.n_heads = 8
    builder.feed_forward_dimensions = 1024
    builder.query_dimensions = 64
    builder.value_dimensions = 64
    builder.dropout = 0.1
    builder.attention_dropout = 0.1
    builder.attention_type = &#34;linear&#34;
    transformer = builder.get()
&#34;&#34;&#34;

__all__ = [
    &#34;AttentionBuilder&#34;,
    &#34;RecurrentAttentionBuilder&#34;,
    &#34;RecurrentCrossAttentionBuilder&#34;
]

# Import the attention implementations so that they register themselves with
# the builder. Attention implementations external to the library should be
# imported before using the builders.
#
# TODO: Should this behaviour change? Namely, should all attention
#       implementations be imported in order to be useable? This also allows
#       using the library even partially built, for instance.
from ..attention import \
    FullAttention, \
    LinearAttention, CausalLinearAttention, \
    ClusteredAttention, ImprovedClusteredAttention, \
    ReformerAttention, \
    ExactTopKAttention, ImprovedClusteredCausalAttention, \
    ConditionalFullAttention
del FullAttention, \
    LinearAttention, CausalLinearAttention, \
    ClusteredAttention, ImprovedClusteredAttention, \
    ReformerAttention, \
    ExactTopKAttention, ImprovedClusteredCausalAttention, \
    ConditionalFullAttention


from .attention_builders import \
    AttentionBuilder, \
    RecurrentAttentionBuilder, \
    RecurrentCrossAttentionBuilder

from .transformer_builders import \
    TransformerEncoderBuilder, \
    RecurrentEncoderBuilder, \
    TransformerDecoderBuilder, \
    RecurrentDecoderBuilder</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="fast_transformers.builders.attention_builders" href="attention_builders.html">fast_transformers.builders.attention_builders</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="fast_transformers.builders.base" href="base.html">fast_transformers.builders.base</a></code></dt>
<dd>
<section class="desc"><p>Provide a class for the others to inherit some useful functionality.</p></section>
</dd>
<dt><code class="name"><a title="fast_transformers.builders.transformer_builders" href="transformer_builders.html">fast_transformers.builders.transformer_builders</a></code></dt>
<dd>
<section class="desc"><p>Build complex transformer architectures for inference or training easily.</p></section>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fast_transformers.builders.AttentionBuilder"><code class="flex name class">
<span>class <span class="ident">AttentionBuilder</span></span>
</code></dt>
<dd>
<section class="desc"><p>Build attention implementations for batch sequence processing or
training.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttentionBuilder(BaseAttentionBuilder):
    &#34;&#34;&#34;Build attention implementations for batch sequence processing or
    training.&#34;&#34;&#34;
    def __init__(self):
        super(AttentionBuilder, self).__init__(AttentionRegistry)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder">BaseAttentionBuilder</a></li>
<li><a title="fast_transformers.builders.base.BaseBuilder" href="base.html#fast_transformers.builders.base.BaseBuilder">BaseBuilder</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder">BaseAttentionBuilder</a></b></code>:
<ul class="hlist">
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.available_attentions" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.available_attentions">available_attentions</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_dictionary" href="base.html#fast_transformers.builders.base.BaseBuilder.from_dictionary">from_dictionary</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_kwargs" href="base.html#fast_transformers.builders.base.BaseBuilder.from_kwargs">from_kwargs</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_namespace" href="base.html#fast_transformers.builders.base.BaseBuilder.from_namespace">from_namespace</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.get" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.get">get</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.validate_attention_type" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.validate_attention_type">validate_attention_type</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fast_transformers.builders.RecurrentAttentionBuilder"><code class="flex name class">
<span>class <span class="ident">RecurrentAttentionBuilder</span></span>
</code></dt>
<dd>
<section class="desc"><p>Build attention implementations for autoregressive sequence
processing.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RecurrentAttentionBuilder(BaseAttentionBuilder):
    &#34;&#34;&#34;Build attention implementations for autoregressive sequence
    processing.&#34;&#34;&#34;
    def __init__(self):
        super(RecurrentAttentionBuilder, self).__init__(
            RecurrentAttentionRegistry
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder">BaseAttentionBuilder</a></li>
<li><a title="fast_transformers.builders.base.BaseBuilder" href="base.html#fast_transformers.builders.base.BaseBuilder">BaseBuilder</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder">BaseAttentionBuilder</a></b></code>:
<ul class="hlist">
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.available_attentions" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.available_attentions">available_attentions</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_dictionary" href="base.html#fast_transformers.builders.base.BaseBuilder.from_dictionary">from_dictionary</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_kwargs" href="base.html#fast_transformers.builders.base.BaseBuilder.from_kwargs">from_kwargs</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_namespace" href="base.html#fast_transformers.builders.base.BaseBuilder.from_namespace">from_namespace</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.get" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.get">get</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.validate_attention_type" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.validate_attention_type">validate_attention_type</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fast_transformers.builders.RecurrentCrossAttentionBuilder"><code class="flex name class">
<span>class <span class="ident">RecurrentCrossAttentionBuilder</span></span>
</code></dt>
<dd>
<section class="desc"><p>Build attention implementations for autoregressive cross attention
computation.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RecurrentCrossAttentionBuilder(BaseAttentionBuilder):
    &#34;&#34;&#34;Build attention implementations for autoregressive cross attention
    computation.&#34;&#34;&#34;
    def __init__(self):
        super(RecurrentCrossAttentionBuilder, self).__init__(
            RecurrentCrossAttentionRegistry
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder">BaseAttentionBuilder</a></li>
<li><a title="fast_transformers.builders.base.BaseBuilder" href="base.html#fast_transformers.builders.base.BaseBuilder">BaseBuilder</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder">BaseAttentionBuilder</a></b></code>:
<ul class="hlist">
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.available_attentions" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.available_attentions">available_attentions</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_dictionary" href="base.html#fast_transformers.builders.base.BaseBuilder.from_dictionary">from_dictionary</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_kwargs" href="base.html#fast_transformers.builders.base.BaseBuilder.from_kwargs">from_kwargs</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.from_namespace" href="base.html#fast_transformers.builders.base.BaseBuilder.from_namespace">from_namespace</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.get" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.get">get</a></code></li>
<li><code><a title="fast_transformers.builders.attention_builders.BaseAttentionBuilder.validate_attention_type" href="attention_builders.html#fast_transformers.builders.attention_builders.BaseAttentionBuilder.validate_attention_type">validate_attention_type</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fast_transformers" href="../index.html">fast_transformers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="fast_transformers.builders.attention_builders" href="attention_builders.html">fast_transformers.builders.attention_builders</a></code></li>
<li><code><a title="fast_transformers.builders.base" href="base.html">fast_transformers.builders.base</a></code></li>
<li><code><a title="fast_transformers.builders.transformer_builders" href="transformer_builders.html">fast_transformers.builders.transformer_builders</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fast_transformers.builders.AttentionBuilder" href="#fast_transformers.builders.AttentionBuilder">AttentionBuilder</a></code></h4>
</li>
<li>
<h4><code><a title="fast_transformers.builders.RecurrentAttentionBuilder" href="#fast_transformers.builders.RecurrentAttentionBuilder">RecurrentAttentionBuilder</a></code></h4>
</li>
<li>
<h4><code><a title="fast_transformers.builders.RecurrentCrossAttentionBuilder" href="#fast_transformers.builders.RecurrentCrossAttentionBuilder">RecurrentCrossAttentionBuilder</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.6.dev1+g65b31fd</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>