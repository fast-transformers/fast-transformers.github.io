<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.6.dev1+g65b31fd" />
<title>fast_transformers.masking API documentation</title>
<meta name="description" content="Create types of masks to be used in various places in transformers â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fast_transformers.masking</code></h1>
</header>
<section id="section-intro">
<p>Create types of masks to be used in various places in transformers.</p>
<ul>
<li>Full mask (any key masked for any query)</li>
<li>Length mask (masking out everything after a length)</li>
<li>Triangular causal mask (mask any key succeeding the query)</li>
</ul>
<p>All mask implementations should provide a single interface to be used by the
transformer layers and the attention layers.</p>
<p>NOTE: In all cases the value 1 or True signifies what should be kept and not
what should be deleted/masked.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#
# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
# Written by Angelos Katharopoulos &lt;angelos.katharopoulos@idiap.ch&gt;,
# Apoorv Vyas &lt;avyas@idiap.ch&gt;
#

&#34;&#34;&#34;Create types of masks to be used in various places in transformers.

- Full mask (any key masked for any query)
- Length mask (masking out everything after a length)
- Triangular causal mask (mask any key succeeding the query)

All mask implementations should provide a single interface to be used by the
transformer layers and the attention layers.

NOTE: In all cases the value 1 or True signifies what should be kept and not
      what should be deleted/masked.
&#34;&#34;&#34;

import torch


class BaseMask(object):
    @property
    def bool_matrix(self):
        &#34;&#34;&#34;Return a bool (uint8) matrix with 1s to all places that should be
        kept.&#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def float_matrix(self):
        &#34;&#34;&#34;Return the bool matrix as a float to be used as a multiplicative
        mask for non softmax attentions.&#34;&#34;&#34;
        if not hasattr(self, &#34;_float_matrix&#34;):
            with torch.no_grad():
                self._float_matrix = self.bool_matrix.float()
        return self._float_matrix

    @property
    def lengths(self):
        &#34;&#34;&#34;If the matrix is of the following form
        
            1 1 1 0 0 0 0
            1 0 0 0 0 0 0
            1 1 0 0 0 0 0

        then return it as a vector of integers

            3 1 2.
        &#34;&#34;&#34;
        if not hasattr(self, &#34;_lengths&#34;):
            with torch.no_grad():
                lengths = self.bool_matrix.long().sum(dim=-1)
                # make sure that the mask starts with 1s and continues with 0s
                # this should be changed to something more efficient, however,
                # I chose simplicity over efficiency since the LengthMask class
                # will be used anyway (and the result is cached)
                m = self.bool_matrix.view(-1, self.shape[-1])
                for i, l in enumerate(lengths.view(-1)):
                    if not torch.all(m[i, :l]):
                        raise ValueError(&#34;The mask is not a length mask&#34;)
                self._lengths = lengths
        return self._lengths

    @property
    def shape(self):
        &#34;&#34;&#34;Return the shape of the boolean mask.&#34;&#34;&#34;
        return self.bool_matrix.shape

    @property
    def additive_matrix(self):
        &#34;&#34;&#34;Return a float matrix to be added to an attention matrix before
        softmax.&#34;&#34;&#34;
        if not hasattr(self, &#34;_additive_matrix&#34;):
            with torch.no_grad():
                self._additive_matrix = torch.log(self.bool_matrix.float())
        return self._additive_matrix

    @property
    def additive_matrix_finite(self):
        &#34;&#34;&#34;Same as additive_matrix but with -1e24 instead of infinity.&#34;&#34;&#34;
        if not hasattr(self, &#34;_additive_matrix_finite&#34;):
            with torch.no_grad():
                self._additive_matrix_finite = (
                    (~self.bool_matrix).float() * (-1e24)
                )
        return self._additive_matrix_finite

    @property
    def all_ones(self):
        &#34;&#34;&#34;Return true if the mask is all ones.&#34;&#34;&#34;
        if not hasattr(self, &#34;_all_ones&#34;):
            with torch.no_grad():
                self._all_ones = torch.all(self.bool_matrix)
        return self._all_ones

    @property
    def lower_triangular(self):
        &#34;&#34;&#34;Return true if the attention is a triangular causal mask.&#34;&#34;&#34;
        if not hasattr(self, &#34;_lower_triangular&#34;):
            self._lower_triangular = False
            with torch.no_grad():
                try:
                    lengths = self.lengths
                    if len(lengths.shape) == 1:
                        target = torch.arange(
                            1,
                            len(lengths)+1,
                            device=lengths.device
                        )
                        self._lower_triangular = torch.all(lengths == target)
                except ValueError:
                    pass
        return self._lower_triangular


class FullMask(BaseMask):
    &#34;&#34;&#34;Thin wrapper over a pytorch tensor that provides the BaseMask
    interface.

    The arguments can be given both by keyword arguments and positional
    arguments. To imitate function overloading, the constructor checks the type
    of the first argument and if it is a tensor it treats it as the mask.
    otherwise it assumes that it was the N argument.

    Arguments
    ---------
        mask: The mask as a PyTorch tensor.
        N: The rows of the all True mask to be created if the mask argument is
           not provided.
        M: The columns of the all True mask to be created if the mask argument
           is not provided. If N is given M defaults to N.
        device: The device to create the mask in (defaults to cpu)
    &#34;&#34;&#34;
    def __init__(self, mask=None, N=None, M=None, device=&#34;cpu&#34;):
        # mask is a tensor so we ignore N and M
        if mask is not None and isinstance(mask, torch.Tensor):
            if mask.dtype != torch.bool:
                raise ValueError(&#34;FullMask expects the mask to be bool&#34;)
            with torch.no_grad():
                self._mask = mask.clone()
            return

        # mask is an integer, N is an integer and M is None so assume they were
        # passed as N, M
        if mask is not None and M is None and isinstance(mask, int):
            M = N
            N = mask

        if N is not None:
            M = M or N
            with torch.no_grad():
                self._mask = torch.ones(N, M, dtype=torch.bool, device=device)
            self._all_ones = True
            return

        raise ValueError(&#34;Either mask or N should be provided&#34;)

    @property
    def bool_matrix(self):
        return self._mask


class LengthMask(BaseMask):
    &#34;&#34;&#34;Provide a BaseMask interface for lengths. Mostly to be used with
    sequences of different lengths.
    
    Arguments
    ---------
        lengths: The lengths as a PyTorch long tensor
        max_len: The maximum length for the mask (defaults to lengths.max())
        device: The device to be used for creating the masks (defaults to
                lengths.device)
    &#34;&#34;&#34;
    def __init__(self, lengths, max_len=None, device=None):
        self._device = device or lengths.device
        with torch.no_grad():
            self._lengths = lengths.clone().to(self._device)
        self._max_len = max_len or self._lengths.max()

        self._bool_matrix = None

    @property
    def bool_matrix(self):
        if self._bool_matrix is None:
            with torch.no_grad():
                indices = torch.arange(self._max_len, device=self._device)
                self._bool_matrix = (
                    indices.view(1, -1) &lt; self._lengths.view(-1, 1)
                )
        return self._bool_matrix


class TriangularCausalMask(LengthMask):
    &#34;&#34;&#34;A square matrix with everything masked out above the diagonal.
    
    Arguments
    ---------
        N: The size of the matrix
        device: The device to create the mask in (defaults to cpu)
    &#34;&#34;&#34;
    def __init__(self, N, device=&#34;cpu&#34;):
        lengths = torch.arange(1, N+1, device=device)
        super(TriangularCausalMask, self).__init__(lengths, N, device)
        self._lower_triangular = True</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fast_transformers.masking.BaseMask"><code class="flex name class">
<span>class <span class="ident">BaseMask</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseMask(object):
    @property
    def bool_matrix(self):
        &#34;&#34;&#34;Return a bool (uint8) matrix with 1s to all places that should be
        kept.&#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def float_matrix(self):
        &#34;&#34;&#34;Return the bool matrix as a float to be used as a multiplicative
        mask for non softmax attentions.&#34;&#34;&#34;
        if not hasattr(self, &#34;_float_matrix&#34;):
            with torch.no_grad():
                self._float_matrix = self.bool_matrix.float()
        return self._float_matrix

    @property
    def lengths(self):
        &#34;&#34;&#34;If the matrix is of the following form
        
            1 1 1 0 0 0 0
            1 0 0 0 0 0 0
            1 1 0 0 0 0 0

        then return it as a vector of integers

            3 1 2.
        &#34;&#34;&#34;
        if not hasattr(self, &#34;_lengths&#34;):
            with torch.no_grad():
                lengths = self.bool_matrix.long().sum(dim=-1)
                # make sure that the mask starts with 1s and continues with 0s
                # this should be changed to something more efficient, however,
                # I chose simplicity over efficiency since the LengthMask class
                # will be used anyway (and the result is cached)
                m = self.bool_matrix.view(-1, self.shape[-1])
                for i, l in enumerate(lengths.view(-1)):
                    if not torch.all(m[i, :l]):
                        raise ValueError(&#34;The mask is not a length mask&#34;)
                self._lengths = lengths
        return self._lengths

    @property
    def shape(self):
        &#34;&#34;&#34;Return the shape of the boolean mask.&#34;&#34;&#34;
        return self.bool_matrix.shape

    @property
    def additive_matrix(self):
        &#34;&#34;&#34;Return a float matrix to be added to an attention matrix before
        softmax.&#34;&#34;&#34;
        if not hasattr(self, &#34;_additive_matrix&#34;):
            with torch.no_grad():
                self._additive_matrix = torch.log(self.bool_matrix.float())
        return self._additive_matrix

    @property
    def additive_matrix_finite(self):
        &#34;&#34;&#34;Same as additive_matrix but with -1e24 instead of infinity.&#34;&#34;&#34;
        if not hasattr(self, &#34;_additive_matrix_finite&#34;):
            with torch.no_grad():
                self._additive_matrix_finite = (
                    (~self.bool_matrix).float() * (-1e24)
                )
        return self._additive_matrix_finite

    @property
    def all_ones(self):
        &#34;&#34;&#34;Return true if the mask is all ones.&#34;&#34;&#34;
        if not hasattr(self, &#34;_all_ones&#34;):
            with torch.no_grad():
                self._all_ones = torch.all(self.bool_matrix)
        return self._all_ones

    @property
    def lower_triangular(self):
        &#34;&#34;&#34;Return true if the attention is a triangular causal mask.&#34;&#34;&#34;
        if not hasattr(self, &#34;_lower_triangular&#34;):
            self._lower_triangular = False
            with torch.no_grad():
                try:
                    lengths = self.lengths
                    if len(lengths.shape) == 1:
                        target = torch.arange(
                            1,
                            len(lengths)+1,
                            device=lengths.device
                        )
                        self._lower_triangular = torch.all(lengths == target)
                except ValueError:
                    pass
        return self._lower_triangular</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="fast_transformers.masking.FullMask" href="#fast_transformers.masking.FullMask">FullMask</a></li>
<li><a title="fast_transformers.masking.LengthMask" href="#fast_transformers.masking.LengthMask">LengthMask</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="fast_transformers.masking.BaseMask.additive_matrix"><code class="name">var <span class="ident">additive_matrix</span></code></dt>
<dd>
<section class="desc"><p>Return a float matrix to be added to an attention matrix before
softmax.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def additive_matrix(self):
    &#34;&#34;&#34;Return a float matrix to be added to an attention matrix before
    softmax.&#34;&#34;&#34;
    if not hasattr(self, &#34;_additive_matrix&#34;):
        with torch.no_grad():
            self._additive_matrix = torch.log(self.bool_matrix.float())
    return self._additive_matrix</code></pre>
</details>
</dd>
<dt id="fast_transformers.masking.BaseMask.additive_matrix_finite"><code class="name">var <span class="ident">additive_matrix_finite</span></code></dt>
<dd>
<section class="desc"><p>Same as additive_matrix but with -1e24 instead of infinity.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def additive_matrix_finite(self):
    &#34;&#34;&#34;Same as additive_matrix but with -1e24 instead of infinity.&#34;&#34;&#34;
    if not hasattr(self, &#34;_additive_matrix_finite&#34;):
        with torch.no_grad():
            self._additive_matrix_finite = (
                (~self.bool_matrix).float() * (-1e24)
            )
    return self._additive_matrix_finite</code></pre>
</details>
</dd>
<dt id="fast_transformers.masking.BaseMask.all_ones"><code class="name">var <span class="ident">all_ones</span></code></dt>
<dd>
<section class="desc"><p>Return true if the mask is all ones.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def all_ones(self):
    &#34;&#34;&#34;Return true if the mask is all ones.&#34;&#34;&#34;
    if not hasattr(self, &#34;_all_ones&#34;):
        with torch.no_grad():
            self._all_ones = torch.all(self.bool_matrix)
    return self._all_ones</code></pre>
</details>
</dd>
<dt id="fast_transformers.masking.BaseMask.bool_matrix"><code class="name">var <span class="ident">bool_matrix</span></code></dt>
<dd>
<section class="desc"><p>Return a bool (uint8) matrix with 1s to all places that should be
kept.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bool_matrix(self):
    &#34;&#34;&#34;Return a bool (uint8) matrix with 1s to all places that should be
    kept.&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="fast_transformers.masking.BaseMask.float_matrix"><code class="name">var <span class="ident">float_matrix</span></code></dt>
<dd>
<section class="desc"><p>Return the bool matrix as a float to be used as a multiplicative
mask for non softmax attentions.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def float_matrix(self):
    &#34;&#34;&#34;Return the bool matrix as a float to be used as a multiplicative
    mask for non softmax attentions.&#34;&#34;&#34;
    if not hasattr(self, &#34;_float_matrix&#34;):
        with torch.no_grad():
            self._float_matrix = self.bool_matrix.float()
    return self._float_matrix</code></pre>
</details>
</dd>
<dt id="fast_transformers.masking.BaseMask.lengths"><code class="name">var <span class="ident">lengths</span></code></dt>
<dd>
<section class="desc"><p>If the matrix is of the following form</p>
<pre><code>1 1 1 0 0 0 0
1 0 0 0 0 0 0
1 1 0 0 0 0 0
</code></pre>
<p>then return it as a vector of integers</p>
<pre><code>3 1 2.
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def lengths(self):
    &#34;&#34;&#34;If the matrix is of the following form
    
        1 1 1 0 0 0 0
        1 0 0 0 0 0 0
        1 1 0 0 0 0 0

    then return it as a vector of integers

        3 1 2.
    &#34;&#34;&#34;
    if not hasattr(self, &#34;_lengths&#34;):
        with torch.no_grad():
            lengths = self.bool_matrix.long().sum(dim=-1)
            # make sure that the mask starts with 1s and continues with 0s
            # this should be changed to something more efficient, however,
            # I chose simplicity over efficiency since the LengthMask class
            # will be used anyway (and the result is cached)
            m = self.bool_matrix.view(-1, self.shape[-1])
            for i, l in enumerate(lengths.view(-1)):
                if not torch.all(m[i, :l]):
                    raise ValueError(&#34;The mask is not a length mask&#34;)
            self._lengths = lengths
    return self._lengths</code></pre>
</details>
</dd>
<dt id="fast_transformers.masking.BaseMask.lower_triangular"><code class="name">var <span class="ident">lower_triangular</span></code></dt>
<dd>
<section class="desc"><p>Return true if the attention is a triangular causal mask.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def lower_triangular(self):
    &#34;&#34;&#34;Return true if the attention is a triangular causal mask.&#34;&#34;&#34;
    if not hasattr(self, &#34;_lower_triangular&#34;):
        self._lower_triangular = False
        with torch.no_grad():
            try:
                lengths = self.lengths
                if len(lengths.shape) == 1:
                    target = torch.arange(
                        1,
                        len(lengths)+1,
                        device=lengths.device
                    )
                    self._lower_triangular = torch.all(lengths == target)
            except ValueError:
                pass
    return self._lower_triangular</code></pre>
</details>
</dd>
<dt id="fast_transformers.masking.BaseMask.shape"><code class="name">var <span class="ident">shape</span></code></dt>
<dd>
<section class="desc"><p>Return the shape of the boolean mask.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self):
    &#34;&#34;&#34;Return the shape of the boolean mask.&#34;&#34;&#34;
    return self.bool_matrix.shape</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="fast_transformers.masking.FullMask"><code class="flex name class">
<span>class <span class="ident">FullMask</span></span>
<span>(</span><span>mask=None, N=None, M=None, device='cpu')</span>
</code></dt>
<dd>
<section class="desc"><p>Thin wrapper over a pytorch tensor that provides the BaseMask
interface.</p>
<p>The arguments can be given both by keyword arguments and positional
arguments. To imitate function overloading, the constructor checks the type
of the first argument and if it is a tensor it treats it as the mask.
otherwise it assumes that it was the N argument.</p>
<h2 id="arguments">Arguments</h2>
<pre><code>mask: The mask as a PyTorch tensor.
N: The rows of the all True mask to be created if the mask argument is
   not provided.
M: The columns of the all True mask to be created if the mask argument
   is not provided. If N is given M defaults to N.
device: The device to create the mask in (defaults to cpu)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FullMask(BaseMask):
    &#34;&#34;&#34;Thin wrapper over a pytorch tensor that provides the BaseMask
    interface.

    The arguments can be given both by keyword arguments and positional
    arguments. To imitate function overloading, the constructor checks the type
    of the first argument and if it is a tensor it treats it as the mask.
    otherwise it assumes that it was the N argument.

    Arguments
    ---------
        mask: The mask as a PyTorch tensor.
        N: The rows of the all True mask to be created if the mask argument is
           not provided.
        M: The columns of the all True mask to be created if the mask argument
           is not provided. If N is given M defaults to N.
        device: The device to create the mask in (defaults to cpu)
    &#34;&#34;&#34;
    def __init__(self, mask=None, N=None, M=None, device=&#34;cpu&#34;):
        # mask is a tensor so we ignore N and M
        if mask is not None and isinstance(mask, torch.Tensor):
            if mask.dtype != torch.bool:
                raise ValueError(&#34;FullMask expects the mask to be bool&#34;)
            with torch.no_grad():
                self._mask = mask.clone()
            return

        # mask is an integer, N is an integer and M is None so assume they were
        # passed as N, M
        if mask is not None and M is None and isinstance(mask, int):
            M = N
            N = mask

        if N is not None:
            M = M or N
            with torch.no_grad():
                self._mask = torch.ones(N, M, dtype=torch.bool, device=device)
            self._all_ones = True
            return

        raise ValueError(&#34;Either mask or N should be provided&#34;)

    @property
    def bool_matrix(self):
        return self._mask</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fast_transformers.masking.BaseMask" href="#fast_transformers.masking.BaseMask">BaseMask</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fast_transformers.masking.BaseMask" href="#fast_transformers.masking.BaseMask">BaseMask</a></b></code>:
<ul class="hlist">
<li><code><a title="fast_transformers.masking.BaseMask.additive_matrix" href="#fast_transformers.masking.BaseMask.additive_matrix">additive_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.additive_matrix_finite" href="#fast_transformers.masking.BaseMask.additive_matrix_finite">additive_matrix_finite</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.all_ones" href="#fast_transformers.masking.BaseMask.all_ones">all_ones</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.bool_matrix" href="#fast_transformers.masking.BaseMask.bool_matrix">bool_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.float_matrix" href="#fast_transformers.masking.BaseMask.float_matrix">float_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.lengths" href="#fast_transformers.masking.BaseMask.lengths">lengths</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.lower_triangular" href="#fast_transformers.masking.BaseMask.lower_triangular">lower_triangular</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.shape" href="#fast_transformers.masking.BaseMask.shape">shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fast_transformers.masking.LengthMask"><code class="flex name class">
<span>class <span class="ident">LengthMask</span></span>
<span>(</span><span>lengths, max_len=None, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Provide a BaseMask interface for lengths. Mostly to be used with
sequences of different lengths.</p>
<h2 id="arguments">Arguments</h2>
<pre><code>lengths: The lengths as a PyTorch long tensor
max_len: The maximum length for the mask (defaults to lengths.max())
device: The device to be used for creating the masks (defaults to
        lengths.device)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LengthMask(BaseMask):
    &#34;&#34;&#34;Provide a BaseMask interface for lengths. Mostly to be used with
    sequences of different lengths.
    
    Arguments
    ---------
        lengths: The lengths as a PyTorch long tensor
        max_len: The maximum length for the mask (defaults to lengths.max())
        device: The device to be used for creating the masks (defaults to
                lengths.device)
    &#34;&#34;&#34;
    def __init__(self, lengths, max_len=None, device=None):
        self._device = device or lengths.device
        with torch.no_grad():
            self._lengths = lengths.clone().to(self._device)
        self._max_len = max_len or self._lengths.max()

        self._bool_matrix = None

    @property
    def bool_matrix(self):
        if self._bool_matrix is None:
            with torch.no_grad():
                indices = torch.arange(self._max_len, device=self._device)
                self._bool_matrix = (
                    indices.view(1, -1) &lt; self._lengths.view(-1, 1)
                )
        return self._bool_matrix</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fast_transformers.masking.BaseMask" href="#fast_transformers.masking.BaseMask">BaseMask</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="fast_transformers.masking.TriangularCausalMask" href="#fast_transformers.masking.TriangularCausalMask">TriangularCausalMask</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fast_transformers.masking.BaseMask" href="#fast_transformers.masking.BaseMask">BaseMask</a></b></code>:
<ul class="hlist">
<li><code><a title="fast_transformers.masking.BaseMask.additive_matrix" href="#fast_transformers.masking.BaseMask.additive_matrix">additive_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.additive_matrix_finite" href="#fast_transformers.masking.BaseMask.additive_matrix_finite">additive_matrix_finite</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.all_ones" href="#fast_transformers.masking.BaseMask.all_ones">all_ones</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.bool_matrix" href="#fast_transformers.masking.BaseMask.bool_matrix">bool_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.float_matrix" href="#fast_transformers.masking.BaseMask.float_matrix">float_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.lengths" href="#fast_transformers.masking.BaseMask.lengths">lengths</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.lower_triangular" href="#fast_transformers.masking.BaseMask.lower_triangular">lower_triangular</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.shape" href="#fast_transformers.masking.BaseMask.shape">shape</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="fast_transformers.masking.TriangularCausalMask"><code class="flex name class">
<span>class <span class="ident">TriangularCausalMask</span></span>
<span>(</span><span>N, device='cpu')</span>
</code></dt>
<dd>
<section class="desc"><p>A square matrix with everything masked out above the diagonal.</p>
<h2 id="arguments">Arguments</h2>
<pre><code>N: The size of the matrix
device: The device to create the mask in (defaults to cpu)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TriangularCausalMask(LengthMask):
    &#34;&#34;&#34;A square matrix with everything masked out above the diagonal.
    
    Arguments
    ---------
        N: The size of the matrix
        device: The device to create the mask in (defaults to cpu)
    &#34;&#34;&#34;
    def __init__(self, N, device=&#34;cpu&#34;):
        lengths = torch.arange(1, N+1, device=device)
        super(TriangularCausalMask, self).__init__(lengths, N, device)
        self._lower_triangular = True</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="fast_transformers.masking.LengthMask" href="#fast_transformers.masking.LengthMask">LengthMask</a></li>
<li><a title="fast_transformers.masking.BaseMask" href="#fast_transformers.masking.BaseMask">BaseMask</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="fast_transformers.masking.LengthMask" href="#fast_transformers.masking.LengthMask">LengthMask</a></b></code>:
<ul class="hlist">
<li><code><a title="fast_transformers.masking.LengthMask.additive_matrix" href="#fast_transformers.masking.BaseMask.additive_matrix">additive_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.LengthMask.additive_matrix_finite" href="#fast_transformers.masking.BaseMask.additive_matrix_finite">additive_matrix_finite</a></code></li>
<li><code><a title="fast_transformers.masking.LengthMask.all_ones" href="#fast_transformers.masking.BaseMask.all_ones">all_ones</a></code></li>
<li><code><a title="fast_transformers.masking.LengthMask.bool_matrix" href="#fast_transformers.masking.BaseMask.bool_matrix">bool_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.LengthMask.float_matrix" href="#fast_transformers.masking.BaseMask.float_matrix">float_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.LengthMask.lengths" href="#fast_transformers.masking.BaseMask.lengths">lengths</a></code></li>
<li><code><a title="fast_transformers.masking.LengthMask.lower_triangular" href="#fast_transformers.masking.BaseMask.lower_triangular">lower_triangular</a></code></li>
<li><code><a title="fast_transformers.masking.LengthMask.shape" href="#fast_transformers.masking.BaseMask.shape">shape</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fast_transformers" href="index.html">fast_transformers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fast_transformers.masking.BaseMask" href="#fast_transformers.masking.BaseMask">BaseMask</a></code></h4>
<ul class="">
<li><code><a title="fast_transformers.masking.BaseMask.additive_matrix" href="#fast_transformers.masking.BaseMask.additive_matrix">additive_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.additive_matrix_finite" href="#fast_transformers.masking.BaseMask.additive_matrix_finite">additive_matrix_finite</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.all_ones" href="#fast_transformers.masking.BaseMask.all_ones">all_ones</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.bool_matrix" href="#fast_transformers.masking.BaseMask.bool_matrix">bool_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.float_matrix" href="#fast_transformers.masking.BaseMask.float_matrix">float_matrix</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.lengths" href="#fast_transformers.masking.BaseMask.lengths">lengths</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.lower_triangular" href="#fast_transformers.masking.BaseMask.lower_triangular">lower_triangular</a></code></li>
<li><code><a title="fast_transformers.masking.BaseMask.shape" href="#fast_transformers.masking.BaseMask.shape">shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="fast_transformers.masking.FullMask" href="#fast_transformers.masking.FullMask">FullMask</a></code></h4>
</li>
<li>
<h4><code><a title="fast_transformers.masking.LengthMask" href="#fast_transformers.masking.LengthMask">LengthMask</a></code></h4>
</li>
<li>
<h4><code><a title="fast_transformers.masking.TriangularCausalMask" href="#fast_transformers.masking.TriangularCausalMask">TriangularCausalMask</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.6.dev1+g65b31fd</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>