<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.6.dev1+g65b31fd" />
<title>fast_transformers.attention.clustered_attention API documentation</title>
<meta name="description" content="Implement clustered self attention." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>fast_transformers.attention.clustered_attention</code></h1>
</header>
<section id="section-intro">
<p>Implement clustered self attention.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#
# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
# Written by Angelos Katharopoulos &lt;angelos.katharopoulos@idiap.ch&gt;,
# Apoorv Vyas &lt;avyas@idiap.ch&gt;
#

&#34;&#34;&#34;Implement clustered self attention.&#34;&#34;&#34;

from math import sqrt

import torch
import torch.autograd
from torch.nn import Dropout, Module
from torch.nn.init import normal_

from ..attention_registry import AttentionRegistry, Optional, Float, Int, \
    Bool, EventDispatcherInstance
from ..events import EventDispatcher
from ..masking import FullMask
from ..aggregate import aggregate, broadcast
from ..clustering.hamming import cluster
from ..hashing import compute_hashes


class _GroupQueries(torch.autograd.Function):
    @staticmethod
    def forward(ctx, Q, clusters, counts):
        factors = 1/counts.float()
        q_grouped = aggregate(Q, clusters, factors)
        ctx.save_for_backward(clusters, factors)

        return q_grouped

    @staticmethod
    def backward(ctx, grad_q_grouped):
        clusters, factors = ctx.saved_tensors
        grad_q = broadcast(grad_q_grouped, clusters, factors)

        return grad_q, None, None


class _BroadcastValues(torch.autograd.Function):
    @staticmethod
    def forward(ctx, v_grouped, clusters, counts):
        factors = torch.ones_like(counts, dtype=v_grouped.dtype)
        V = broadcast(v_grouped, clusters, factors)
        ctx.save_for_backward(clusters, factors)

        return V

    @staticmethod
    def backward(ctx, grad_v):
        clusters, factors = ctx.saved_tensors
        grad_v_grouped = aggregate(grad_v, clusters, factors)

        return grad_v_grouped, None, None


class ClusteredAttention(Module):
    &#34;&#34;&#34;Use LSH and clustering in the resulting Hamming space to group queries
    that will have minimal L2 distance from each other.

    Given the queries, keys, and values as Q, K, and V respectively, we
    first cluster the queries in &#34;C&#34; groups and compute the &#34;C&#34; query centroids
    Q_c.

    We now use to the centroids Q_c to compute the attention using:
    
        V&#39;_c = softmax(Q_c.mm(K.t()), dim=-1).mm(V).

    Now the computed values V&#39;_c are &#34;broadcasted&#34; back to the query members
    of the corresponding cluster.

    Arguments
    ---------
        clusters: How many clusters to group the queries into
        iterations: The number of lloyd iterations to perform (default: 10)
        bits: How many bits to use for the hash (default: 32)
        hash_bias: If true, hamming distance proportional to L2 distance
                   If false, hamming distance proportional to cosine distance
                   (default: True)
        softmax_temp: The temperature to use for the softmax attention.
                      (default: 1/sqrt(d_keys) where d_keys is computed at
                      runtime)
        attention_dropout: The dropout rate to apply to the attention
                           (default: 0.1)
        event_dispatcher: str or EventDispatcher instance to be used by this
                          module for dispatching events (default: the default
                          global dispatcher)
    &#34;&#34;&#34;
    def __init__(self, clusters, iterations=10, bits=32,
                 hash_bias=True, softmax_temp=None, attention_dropout=0.1,
                 event_dispatcher=&#34;&#34;):
        super(ClusteredAttention, self).__init__()
        self.clusters = clusters
        self.iterations = iterations
        self.bits = bits
        self.hash_bias = hash_bias
        self.softmax_temp = softmax_temp
        self.dropout = Dropout(attention_dropout)
        self.event_dispatcher = EventDispatcher.get(event_dispatcher)

    def _create_query_groups(self, Q, query_lengths):
        N, H, L, E = Q.shape

        # Compute the hashes for all the queries
        planes = Q.new_empty((self.bits, E+1))
        normal_(planes)
        if not self.hash_bias:
            planes[:, -1] = 0
        hashes = compute_hashes(Q.view(N*H*L, E), planes).view(N, H, L)

        # Cluster the hashes and return the cluster index per query
        groups =  cluster(
            hashes,
            query_lengths._lengths.int(),
            clusters=self.clusters,
            iterations=self.iterations,
            bits=self.bits
        )
        return groups

    def _group_queries(self, Q, groups):
        &#34;&#34;&#34;Aggregate the Qs based on the index of cluster they belong to. Make
        sure to allow for gradient propagation backwards from the grouped
        queries to each query.&#34;&#34;&#34;
        q_grouped = _GroupQueries.apply(Q, *groups)
        return q_grouped

    def _broadcast_values(self, V, groups):
        &#34;&#34;&#34;Broadcast the values back to the correct positions but make sure
        that the gradient flows properly.&#34;&#34;&#34;
        V_new = _BroadcastValues.apply(V.contiguous(), *groups)
        V_new = V_new.permute(0, 2, 1, 3).contiguous()
        return V_new

    def forward(self, queries, keys, values, attn_mask, query_lengths,
                key_lengths):
        # Make sure that there is no attention mask
        assert attn_mask.all_ones, (&#34;Clustered attention cannot use an &#34;
                                    &#34;arbitrary attention mask.&#34;)

        queries = queries.permute(0,2,1,3).contiguous()
        keys = keys.permute(0,2,1,3).contiguous()
        values = values.permute(0,2,1,3).contiguous()

        N, H, L, E = queries.shape
        softmax_temp = self.softmax_temp or 1./sqrt(E)
        
        # Cluster the queries into groups
        groups = self._create_query_groups(queries, query_lengths)
        Q_grouped = self._group_queries(queries, groups)
        # Compute the attention
        QK = torch.einsum(&#34;nhle,nhse-&gt;nhls&#34;, Q_grouped, keys)
        QK = QK + key_lengths.additive_matrix[:, None, None, :]
        A = self.dropout(torch.softmax(softmax_temp * QK, dim=-1))
        V = torch.einsum(&#34;nhls,nhsd-&gt;nhld&#34;, A, values)

        # Broadcast grouped attention
        return self._broadcast_values(V, groups)


# Register the attention implementation so that it becomes available in our
# builders
AttentionRegistry.register(
    &#34;clustered&#34;, ClusteredAttention,
    [
        (&#34;clusters&#34;, Int),
        (&#34;iterations&#34;, Optional(Int, 10)),
        (&#34;bits&#34;, Optional(Int, 32)),
        (&#34;hash_bias&#34;, Optional(Bool, True)),
        (&#34;softmax_temp&#34;, Optional(Float)),
        (&#34;attention_dropout&#34;, Optional(Float, 0.1)),
        (&#34;event_dispatcher&#34;, Optional(EventDispatcherInstance, &#34;&#34;))
    ]
)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="fast_transformers.attention.clustered_attention.ClusteredAttention"><code class="flex name class">
<span>class <span class="ident">ClusteredAttention</span></span>
<span>(</span><span>clusters, iterations=10, bits=32, hash_bias=True, softmax_temp=None, attention_dropout=0.1, event_dispatcher='')</span>
</code></dt>
<dd>
<section class="desc"><p>Use LSH and clustering in the resulting Hamming space to group queries
that will have minimal L2 distance from each other.</p>
<p>Given the queries, keys, and values as Q, K, and V respectively, we
first cluster the queries in "C" groups and compute the "C" query centroids
Q_c.</p>
<p>We now use to the centroids Q_c to compute the attention using:</p>
<pre><code>V'_c = softmax(Q_c.mm(K.t()), dim=-1).mm(V).
</code></pre>
<p>Now the computed values V'_c are "broadcasted" back to the query members
of the corresponding cluster.</p>
<h2 id="arguments">Arguments</h2>
<pre><code>clusters: How many clusters to group the queries into
iterations: The number of lloyd iterations to perform (default: 10)
bits: How many bits to use for the hash (default: 32)
hash_bias: If true, hamming distance proportional to L2 distance
           If false, hamming distance proportional to cosine distance
           (default: True)
softmax_temp: The temperature to use for the softmax attention.
              (default: 1/sqrt(d_keys) where d_keys is computed at
              runtime)
attention_dropout: The dropout rate to apply to the attention
                   (default: 0.1)
event_dispatcher: str or EventDispatcher instance to be used by this
                  module for dispatching events (default: the default
                  global dispatcher)
</code></pre>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClusteredAttention(Module):
    &#34;&#34;&#34;Use LSH and clustering in the resulting Hamming space to group queries
    that will have minimal L2 distance from each other.

    Given the queries, keys, and values as Q, K, and V respectively, we
    first cluster the queries in &#34;C&#34; groups and compute the &#34;C&#34; query centroids
    Q_c.

    We now use to the centroids Q_c to compute the attention using:
    
        V&#39;_c = softmax(Q_c.mm(K.t()), dim=-1).mm(V).

    Now the computed values V&#39;_c are &#34;broadcasted&#34; back to the query members
    of the corresponding cluster.

    Arguments
    ---------
        clusters: How many clusters to group the queries into
        iterations: The number of lloyd iterations to perform (default: 10)
        bits: How many bits to use for the hash (default: 32)
        hash_bias: If true, hamming distance proportional to L2 distance
                   If false, hamming distance proportional to cosine distance
                   (default: True)
        softmax_temp: The temperature to use for the softmax attention.
                      (default: 1/sqrt(d_keys) where d_keys is computed at
                      runtime)
        attention_dropout: The dropout rate to apply to the attention
                           (default: 0.1)
        event_dispatcher: str or EventDispatcher instance to be used by this
                          module for dispatching events (default: the default
                          global dispatcher)
    &#34;&#34;&#34;
    def __init__(self, clusters, iterations=10, bits=32,
                 hash_bias=True, softmax_temp=None, attention_dropout=0.1,
                 event_dispatcher=&#34;&#34;):
        super(ClusteredAttention, self).__init__()
        self.clusters = clusters
        self.iterations = iterations
        self.bits = bits
        self.hash_bias = hash_bias
        self.softmax_temp = softmax_temp
        self.dropout = Dropout(attention_dropout)
        self.event_dispatcher = EventDispatcher.get(event_dispatcher)

    def _create_query_groups(self, Q, query_lengths):
        N, H, L, E = Q.shape

        # Compute the hashes for all the queries
        planes = Q.new_empty((self.bits, E+1))
        normal_(planes)
        if not self.hash_bias:
            planes[:, -1] = 0
        hashes = compute_hashes(Q.view(N*H*L, E), planes).view(N, H, L)

        # Cluster the hashes and return the cluster index per query
        groups =  cluster(
            hashes,
            query_lengths._lengths.int(),
            clusters=self.clusters,
            iterations=self.iterations,
            bits=self.bits
        )
        return groups

    def _group_queries(self, Q, groups):
        &#34;&#34;&#34;Aggregate the Qs based on the index of cluster they belong to. Make
        sure to allow for gradient propagation backwards from the grouped
        queries to each query.&#34;&#34;&#34;
        q_grouped = _GroupQueries.apply(Q, *groups)
        return q_grouped

    def _broadcast_values(self, V, groups):
        &#34;&#34;&#34;Broadcast the values back to the correct positions but make sure
        that the gradient flows properly.&#34;&#34;&#34;
        V_new = _BroadcastValues.apply(V.contiguous(), *groups)
        V_new = V_new.permute(0, 2, 1, 3).contiguous()
        return V_new

    def forward(self, queries, keys, values, attn_mask, query_lengths,
                key_lengths):
        # Make sure that there is no attention mask
        assert attn_mask.all_ones, (&#34;Clustered attention cannot use an &#34;
                                    &#34;arbitrary attention mask.&#34;)

        queries = queries.permute(0,2,1,3).contiguous()
        keys = keys.permute(0,2,1,3).contiguous()
        values = values.permute(0,2,1,3).contiguous()

        N, H, L, E = queries.shape
        softmax_temp = self.softmax_temp or 1./sqrt(E)
        
        # Cluster the queries into groups
        groups = self._create_query_groups(queries, query_lengths)
        Q_grouped = self._group_queries(queries, groups)
        # Compute the attention
        QK = torch.einsum(&#34;nhle,nhse-&gt;nhls&#34;, Q_grouped, keys)
        QK = QK + key_lengths.additive_matrix[:, None, None, :]
        A = self.dropout(torch.softmax(softmax_temp * QK, dim=-1))
        V = torch.einsum(&#34;nhls,nhsd-&gt;nhld&#34;, A, values)

        # Broadcast grouped attention
        return self._broadcast_values(V, groups)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="fast_transformers.attention.clustered_attention.ClusteredAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, queries, keys, values, attn_mask, query_lengths, key_lengths)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, queries, keys, values, attn_mask, query_lengths,
            key_lengths):
    # Make sure that there is no attention mask
    assert attn_mask.all_ones, (&#34;Clustered attention cannot use an &#34;
                                &#34;arbitrary attention mask.&#34;)

    queries = queries.permute(0,2,1,3).contiguous()
    keys = keys.permute(0,2,1,3).contiguous()
    values = values.permute(0,2,1,3).contiguous()

    N, H, L, E = queries.shape
    softmax_temp = self.softmax_temp or 1./sqrt(E)
    
    # Cluster the queries into groups
    groups = self._create_query_groups(queries, query_lengths)
    Q_grouped = self._group_queries(queries, groups)
    # Compute the attention
    QK = torch.einsum(&#34;nhle,nhse-&gt;nhls&#34;, Q_grouped, keys)
    QK = QK + key_lengths.additive_matrix[:, None, None, :]
    A = self.dropout(torch.softmax(softmax_temp * QK, dim=-1))
    V = torch.einsum(&#34;nhls,nhsd-&gt;nhld&#34;, A, values)

    # Broadcast grouped attention
    return self._broadcast_values(V, groups)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="fast_transformers.attention" href="index.html">fast_transformers.attention</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="fast_transformers.attention.clustered_attention.ClusteredAttention" href="#fast_transformers.attention.clustered_attention.ClusteredAttention">ClusteredAttention</a></code></h4>
<ul class="">
<li><code><a title="fast_transformers.attention.clustered_attention.ClusteredAttention.forward" href="#fast_transformers.attention.clustered_attention.ClusteredAttention.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.6.dev1+g65b31fd</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>