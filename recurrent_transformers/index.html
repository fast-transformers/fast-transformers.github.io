<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Recurrent Transformers - Fast Transformers for PyTorch</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Recurrent Transformers";
    var mkdocs_page_input_path = "recurrent_transformers.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Fast Transformers for PyTorch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../transformers/">Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../masking/">Masking</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../attention/">Attention</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../builders/">Builders</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../custom_attention_layer/">Custom Attention Layer</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Recurrent Transformers</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#forward-method">Forward method</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#available-attentions">Available Attentions</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#example">Example</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="" href="/api_docs/fast_transformers/">API Docs</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Fast Transformers for PyTorch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Recurrent Transformers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/idiap/fast-transformers/edit/master/docs/recurrent_transformers.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="recurrent-transformers">Recurrent Transformers</h1>
<p>The transformer layers implemented in the <a href="/api_docs/fast_transformers/transformers.html">fast_transformers.transformers</a>
module are processing the entire sequence simultaneously. On the other hand,
this module implements transfomers as recurrent networks. Namely as networks
that process the sequence one element at a time while updating some state.</p>
<p>The TransformerEncoder and TransformerEncoderLayer give way to
<a href="/api_docs/fast_transformers/recurrent/transformers.html#fast_transformers.recurrent.transformers.RecurrentTransformerEncoder">RecurrentTransformerEncoder</a> and <a href="/api_docs/fast_transformers/recurrent/transformers.html#fast_transformers.recurrent.transformers.RecurrentTransformerEncoderLayer">RecurrentTransformerEncoderLayer</a> and
for the decoders <a href="/api_docs/fast_transformers/recurrent/transformers.html#fast_transformers.recurrent.transformers.RecurrentTransformerDecoder">RecurrentTransformerDecoder</a> and
<a href="/api_docs/fast_transformers/recurrent/transformers.html#fast_transformers.recurrent.transformers.RecurrentTransformerDecoderLayer">RecurrentTransformerDecoderLayer</a> respectively.</p>
<h2 id="forward-method">Forward method</h2>
<p><strong>RecurrentTransformerEncoder</strong> or <strong>RecurrentTransformerEncoderLayer</strong></p>
<pre><code>forward(x, state=None)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: The input features of shape (N, E) where N is the batch size and E is
  <code>d_model</code> passed in the constructor. Note that x corresponds to a specific
  element in the sequence and not the entire sequence.</li>
<li><strong>state</strong>: The state is a python object that varies depending on the
  attention implementation</li>
</ul>
<p><strong>RecurrentTransformerDecoder</strong> or <strong>RecurrentTransformerDecoderLayer</strong></p>
<pre><code>forward(x, memory, memory_length_mask=None, state=None)
</code></pre>

<ul>
<li><strong>x</strong>: The input features of shape (N, E) where N is the batch size and E is
  <code>d_model</code> passed in the constructor. Note that x corresponds to a specific
  element in the sequence and not the entire sequence.</li>
<li><strong>memory</strong>: A sequence of features (N, S, E) that the input will attend
  to. S is the sequence length and E is the same as for x.</li>
<li><strong>memory_length_mask</strong>: An implementation of a BaseMask that encodes
  how many elements each memory sequence in the batch consists of.</li>
<li><strong>state</strong>: The state is a python object that varies depending on the
  attention implementation</li>
</ul>
<div class="admonition note">
    <p class="admonition-title">Note</p>
    <p>The masks are different in the recurrent implementations than in their
    batch counterparts. Namely, recurrent encoders and decoders enforce a
    triangular causal mask on self attention. In addition, recurrent decoders
    enforce a full mask on cross attention.</p>
</div>

<h2 id="available-attentions">Available Attentions</h2>
<p>Not all attention formulations can be written in an autoregressive fashion as a
recurrent model. In particular, since the sequence is passed to the transformer
element by element we have the same result as passing a causal mask to normal
transformers. The current list for recurrent attention implementations is:</p>
<ul>
<li><a href="/api_docs/fast_transformers/recurrent/attention/self_attention/linear_attention.html">LinearAttention</a></li>
<li><a href="/api_docs/fast_transformers/recurrent/attention/self_attention/full_attention.html">FullAttention</a></li>
</ul>
<h2 id="example">Example</h2>
<p>The following example builds a random recurrent transformer encoder and applies
its output as input 100 times.</p>
<pre><code class="python"># for simplicity ignore all the classification
# layers and the embedding layers

from fast_transformers.builders import RecurrentEncoderBuilder

model = RecurrentEncoderBuilder.from_kwargs(
    attention_type=&quot;linear&quot;,
    n_layers=8,
    n_heads=12,
    feed_forward_dimensions=1536,
    query_dimensions=32,
    value_dimensions=32
).get()

x0 = torch.rand(
    10,    # batch size
    12*32  # feature size
)
state = None

x = x0
for i in range(100):
    x, state = model(x, state=state)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../custom_attention_layer/" class="btn btn-neutral" title="Custom Attention Layer"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/idiap/fast-transformers/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../custom_attention_layer/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
