<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Masking - Fast Transformers for PyTorch</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Masking";
    var mkdocs_page_input_path = "masking.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Fast Transformers for PyTorch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../transformers/">Transformers</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Masking</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#basemask">BaseMask</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#implementations">Implementations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#fullmask">FullMask</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lengthmask">LengthMask</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#triangularcausalmask">TriangularCausalMask</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../attention/">Attention</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../builders/">Builders</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../custom_attention_layer/">Custom Attention Layer</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../recurrent_transformers/">Recurrent Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="" href="/api_docs/fast_transformers/">API Docs</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Fast Transformers for PyTorch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Masking</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/idiap/fast-transformers/edit/master/docs/masking.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="masking">Masking</h1>
<p>In this library, both for convenience and efficiency, we define a <a href="/api_docs/fast_transformers/masking.html#fast_transformers.masking.BaseMask">BaseMask</a>
interface that all masks should implement. The BaseMask interface allows
accessing a mask in the following ways:</p>
<ol>
<li>a bool tensor where True signifies what is kept</li>
<li>a float tensor where minus infinity signifies what is to be masked</li>
<li>a float tensor where zero signifies what is to be masked</li>
<li>a length tensor where everything after a certain length is to be masked</li>
</ol>
<p>This interface allows us to use the same mask definition with various attention
implementations without compromising in performance or requiring code changes.
For instance, softmax masks are usually implemented with additive masks that
contain -inf and linear attention masks are efficiently implemented with
multiplicative masks that contain zeros.</p>
<h2 id="basemask">BaseMask</h2>
<p>Our <a href="/api_docs/fast_transformers/masking.html#fast_transformers.masking.BaseMask">API docs</a> are quite thorough in explaining the BaseMask interface.</p>
<h2 id="implementations">Implementations</h2>
<p>We provide three implementations of the BaseMask interface <em>FullMask</em>,
<em>LengthMask</em> and <em>TriangularCausalMask</em>.</p>
<h3 id="fullmask">FullMask</h3>
<pre><code>fast_transformers.masking.FullMask(mask=None, N=None, M=None, device='cpu')
</code></pre>

<p>The FullMask is a simple wrapper over a pytorch boolean tensor. The arguments
can be given both by keyword arguments and positional arguments. To imitate
function overloading, the constructor checks the type of the first argument and
if it is a tensor it treats it as the mask. otherwise it assumes that it was
the N argument.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>mask</strong>: The mask as a PyTorch tensor.</li>
<li><strong>N</strong>: The rows of the all True mask to be created if the mask argument is
  not provided.</li>
<li><strong>M</strong>: The columns of the all True mask to be created if the mask argument
  is not provided. If N is given M defaults to N.</li>
<li><strong>device</strong>: The device to create the mask in (defaults to cpu)</li>
</ul>
<h3 id="lengthmask">LengthMask</h3>
<pre><code>fast_transformers.masking.LengthMask(lengths, max_len=None, device=None)
</code></pre>

<p>The LengthMask is designed to be used for conveying different lengths of
sequences. It can be accessed as an array of integers which may be beneficial
for some attention implementations.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>lengths</strong>: The lengths as a PyTorch long tensor</li>
<li><strong>max_len</strong>: The maximum length for the mask (defaults to lengths.max())</li>
<li><strong>device</strong>: The device to be used for creating the masks (defaults to
  lengths.device)</li>
</ul>
<h3 id="triangularcausalmask">TriangularCausalMask</h3>
<pre><code>fast_transformers.masking.TriangularCausalMask(N, device=&quot;cpu&quot;)
</code></pre>

<p>Represents a square matrix with everything masked above the main diagonal. It
is meant to be used for training autoregressive transformers.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>N</strong>: The size of the matrix</li>
<li><strong>device</strong>: The device to create the mask in (defaults to cpu)</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../attention/" class="btn btn-neutral float-right" title="Attention">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../transformers/" class="btn btn-neutral" title="Transformers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/idiap/fast-transformers/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../transformers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../attention/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
