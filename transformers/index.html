<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Transformers - Fast Transformers for PyTorch</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Transformers";
    var mkdocs_page_input_path = "transformers.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Fast Transformers for PyTorch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Transformers</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#forward-method">Forward method</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#transformerencoder">TransformerEncoder</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#transformerencoderlayer">TransformerEncoderLayer</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../masking/">Masking</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../attention/">Attention</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../builders/">Builders</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../custom_attention_layer/">Custom Attention Layer</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../recurrent_transformers/">Recurrent Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="" href="/api_docs/fast_transformers/">API Docs</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Fast Transformers for PyTorch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Transformers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/idiap/fast-transformers/edit/master/docs/transformers.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="transformers">Transformers</h1>
<p>The <a href="/api_docs/fast_transformers/transformers.html">fast_transformers.transformers</a>
module provides the <code>TransformerEncoder</code> and <code>TransformerEncoderLayer</code> classes
that implement a common transformer encoder similar to the PyTorch API.</p>
<p>However, an important difference is that the <code>TransformerEncoder</code> <strong>does not
create</strong> the <code>TransformerEncoderLayer</code> which allows for injecting a different
implementation with minimal code changes. The encoder layer follows the same
principle and does not create the attention layer but receives it as an
argument which allows for using many different attention implementations with
an otherwise identical model.</p>
<p>We also provide <a href="../recurrent_transformers/">recurrent transformer encoders</a>
which are meant to be given each input one at a time for autoregressive
inference.</p>
<h2 id="forward-method">Forward method</h2>
<p>Both the transformer encoder and the transformer encoder layer accept the same
parameters for the <code>forward()</code> method.</p>
<pre><code>forward(x, attn_mask=None, length_mask=None)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: The input features of shape (N, L, E) where N is the batch size,
  L is the sequence length (padded) and E is <code>d_model</code> passed in the
  constructor.</li>
<li><strong>attn_mask</strong>: An implementation of
  <a href="../masking/">fast_transformers.masking.BaseMask</a> that encodes where each
  element of x can attend to.</li>
<li><strong>length_mask</strong>: An implementation of
  <a href="../masking/">fast_transformers.masking.BaseMask</a> that encodes how many
  elements each sequence in the batch consists of.</li>
</ul>
<p>If the masks are not provided they are automatically created as an all ones
mask for the attention mask and the size of the tensor for the length mask.</p>
<div class="admonition note">
    <p class="admonition-title">Note</p>
    <p>Unlike the PyTorch transformer the dimensions of the input are ordered
       with the <b>batch size first and the sequence second</b>.</p>
</div>

<h2 id="transformerencoder">TransformerEncoder</h2>
<pre><code>fast_transformers.transformers.TransformerEncoder(layers, norm_layer=None)
</code></pre>

<p>The TransformerEncoder is simply a container for transformer encoder layers
that it receives as a list upon construction. Simply put it is a Sequential
that is aware of <a href="../masking/">masking</a> and passes the masks to all the
transformer encoder layers.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>layers</strong>: A list of TransformerEncoderLayer instances or other nn.Module
  instances that implement the same interface</li>
<li><strong>norm_layer</strong>: A normalization layer to be applied to the final output
  (default: None which means no normalization)</li>
</ul>
<h2 id="transformerencoderlayer">TransformerEncoderLayer</h2>
<pre><code>fast_transformers.transformers.TransformerEncoderLayer(attention, d_model, n_heads, d_ff=None, dropout=0.1, activation='relu')
</code></pre>

<p>This transformer encoder layer implements the same encoder layer as PyTorch but
is a bit more open for extension by receiving the attention implementation as a
constructor argument.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>attention</strong>: The attention implementation to use given as a nn.Module</li>
<li><strong>d_model</strong>: The input feature dimensionality</li>
<li><strong>n_heads</strong>: The number of heads for the multi head attention</li>
<li><strong>d_ff</strong>: The dimensionality of the intermediate features after the
  attention (default: d_model*4)</li>
<li><strong>dropout</strong>: The dropout rate to apply to the intermediate features
  (default: 0.1)</li>
<li><strong>activation</strong>: Choose which activation to use for the feed
  forward part of the layer from the set {'relu', 'gelu'} (default: relu)</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../masking/" class="btn btn-neutral float-right" title="Masking">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href=".." class="btn btn-neutral" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/idiap/fast-transformers/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../masking/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
