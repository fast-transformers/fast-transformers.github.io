<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Custom Attention Layer - Fast Transformers for PyTorch</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Custom Attention Layer";
    var mkdocs_page_input_path = "custom_attention_layer.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Fast Transformers for PyTorch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../transformers/">Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../masking/">Masking</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../attention/">Attention</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../builders/">Builders</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Custom Attention Layer</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#new-attention">New Attention</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#integrate-with-the-builder">Integrate with the Builder</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#future-changes">Future changes</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../recurrent_transformers/">Recurrent Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="" href="/api_docs/fast_transformers/">API Docs</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Fast Transformers for PyTorch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Custom Attention Layer</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/idiap/fast-transformers/edit/master/docs/custom_attention_layer.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="creating-a-custom-attention-layer">Creating a custom attention layer</h1>
<p>In this page, we will go through the process of creating a custom attention
module and integrating it with the library. We will implement a quadratic
kernel attention instead of softmax attention.</p>
<h2 id="new-attention">New Attention</h2>
<p>Our attention layer will follow closely the implementation of
<a href="https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/full_attention.py">FullAttention</a>. Let's start with the skeleton of our module.</p>
<pre><code class="python">class QuadraticAttention(Module):
    def __init__(self, eps=1e-6):
        super(QuadraticAttention, self).__init__()
        self.eps = eps

    def forward(self, queries, keys, values, attn_mask, query_lengths,
                key_lengths):
        # implement the logic of the layer here
</code></pre>

<p>The queries, keys and values are already projected and split into multiple
heads by the <a href="../attention/">AttentionLayer</a>. This means that we need only implement the
attention part.</p>
<pre><code class="python">class QuadraticAttention(Module):
    def __init__(self, eps=1e-6):
        super(QuadraticAttention, self).__init__()
        self.eps = eps

    def forward(self, queries, keys, values, attn_mask, query_lengths,
                key_lengths):
        # compute the unnormalized attention
        QK = torch.einsum(&quot;nlhe,nshe-&gt;nhls&quot;, queries, keys) # compute the dot products
        QK = torch.square(QK) # implement our custom attention twist
        QK = QK * attn_mask.float_matrix # use the attention mask as a multiplicative mask
        QK = QK * key_lengths.float_matrix[:, None, None] # also a multiplicative mask

        # normalize and compute the average
        A = QK / (QK.sum(dim=-1, keepdim=True) + self.eps)
        V = torch.einsum(&quot;nhls,nshd-&gt;nlhd&quot;, A, values)

        return V.contiguous()
</code></pre>

<h2 id="integrate-with-the-builder">Integrate with the Builder</h2>
<p>To add it as an option to the <code>TransformerEncoderBuilder</code> we have to edit two
files. Firstly, we have to add it as an option to the
<a href="https://github.com/idiap/fast-transformers/blob/master/fast_transformers/builders/attention_builder.py">AttentionBuilder.attention_type</a> as follows:</p>
<pre><code class="python">class AttentionBuilder(object):
    ...
    ...
    @attention_type.setter
    def attention_type(self, val):
        attentions = [&quot;full&quot;, &quot;clustered&quot;, &quot;improved-clustered&quot;,
                      &quot;improved-causal&quot;, &quot;linear&quot;, &quot;causal-linear&quot;,
                      &quot;reformer&quot;, &quot;exact-topk&quot;, &quot;square&quot;] # add the 'square' to the list
        if val not in attentions:
            raise ValueError((&quot;{!r} is not one of the available attention &quot;
                              &quot;types {!r}&quot;).format(val, attentions))
        self._attention_type = val
    ...
    ...
</code></pre>

<p>Secondly, we have to edit the <a href="https://github.com/idiap/fast-transformers/blob/master/fast_transformers/builders/transformer_encoder_builder.py">TransformerEncoderBuilder</a> to create the
correct attention layer when the attention_type is set to 'square'.</p>
<pre><code class="python">class TransformerEncoderBuilder(BaseTransformerBuilder):
    ...
    ...
    def _get_attention(self):
        attentions = {
            ...
            ...
            &quot;square&quot;: QuadraticAttention
        }
        ...
        ...
</code></pre>

<p>After those changes we can use the builder to create transformers with our new
attention layer.</p>
<pre><code class="python">quadratic_bert = TransformerEncoderBuilder.from_kwargs(
    attention_type=&quot;square&quot;, # here we select our custom attention layer
    n_layers=12,
    n_heads=12,
    query_dimensions=64,
    value_dimensions=64,
    feed_forward_dimensions=3072,
    activation=&quot;gelu&quot;
)
</code></pre>

<h2 id="future-changes">Future changes</h2>
<p>Editing two files to add a new attention implementation to a transformer
builder is obviously suboptimal. We are in the process of changing this
procedure with a more streamlined way of registering new attention layers to
the transformer builders.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../recurrent_transformers/" class="btn btn-neutral float-right" title="Recurrent Transformers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../builders/" class="btn btn-neutral" title="Builders"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/idiap/fast-transformers/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../builders/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../recurrent_transformers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
