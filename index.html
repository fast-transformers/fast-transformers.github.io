<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  
  <link rel="shortcut icon" href="img/favicon.ico">
  <title>Fast Transformers for PyTorch</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="css/theme.css" />
  <link rel="stylesheet" href="css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Home";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="js/jquery-2.1.1.min.js" defer></script>
  <script src="js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> Fast Transformers for PyTorch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href=".">Home</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#quick-start">Quick-start</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#research">Research</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#ours">Ours</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#by-others">By others</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#support-license-and-copyright">Support, License and Copyright</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="transformers/">Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="masking/">Masking</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="attention/">Attention</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="builders/">Builders</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="custom_attention_layer/">Custom Attention Layer</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="recurrent_transformers/">Recurrent Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="" href="/api_docs/fast_transformers/">API Docs</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">Fast Transformers for PyTorch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/idiap/fast-transformers/edit/master/docs/index.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="fast-transformers">Fast Transformers</h1>
<p>Transformers are very succsessfull models that achieve state of the art
performance in many natural language tasks. However, it is very difficult to
scale them to long sequences due to the quadratic scaling of the
self-attention.</p>
<p>This library was developed for our research on fast attention for transformers.
You can find a list of our papers <a href="#research">below</a> as well as related papers
and papers that we have implemented.</p>
<h2 id="quick-start">Quick-start</h2>
<p>The main interface of the library for using the implemented fast transformers
is the <a href="api/fast_transformers/builders/">builder interface</a>. This allows for
experimenting with different attention implentations with minimal code changes.
For instance building a BERT-like transformer encoder is as simple as the
following code:</p>
<pre><code class="python">import torch
from fast_transformers.builders import TransformerEncoderBuilder

# Build a transformer encoder
bert = TransformerEncoderBuilder.from_kwargs(
    n_layers=12,
    n_heads=12,
    query_dimensions=64,
    value_dimensions=64,
    feed_forward_dimensions=3072,
    attention_type=&quot;full&quot;, # change this to use another
                           # attention implementation
    activation=&quot;gelu&quot;
).get()

y = bert(torch.rand(
    10,    # batch_size
    512,   # sequence length
    64*12  # features
))
</code></pre>

<h2 id="installation">Installation</h2>
<p>The fast transformers library has the following dependencies:</p>
<ul>
<li>PyTorch</li>
<li>C++ toolchain</li>
<li>CUDA toolchain (if you want to compile for GPUs)</li>
</ul>
<p>For most machines installation should be as simple as:</p>
<pre><code class="bash">pip install --user fast_transformers
</code></pre>

<h2 id="research">Research</h2>
<h3 id="ours">Ours</h3>
<p>To read about the theory behind some attention implementations in this library
we encourage you to follow our research.</p>
<ul>
<li>Transformers are RNNs: Fast Autoregressive Transformers with
  Linear Attention (<a href="https://arxiv.org/abs/2006.16236">arxiv</a>,
  <a href="https://youtu.be/KBWh7XCUAi8">video</a>)</li>
<li>Fast Transformers with Clustered Attention (preprint coming soon)</li>
</ul>
<p>If you found our research helpful or influential please consider citing</p>
<pre><code>@inproceedings{katharopoulos_et_al_2020,
    author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},
    title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
    booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
    year = {2020}
}
</code></pre>

<h3 id="by-others">By others</h3>
<ul>
<li>Efficient Attention: Attention with Linear Complexities (<a href="https://arxiv.org/abs/1812.01243">arxiv</a>)</li>
<li>Linformer: Self-Attention with Linear Complexity (<a href="https://arxiv.org/abs/2006.04768">arxiv</a>)</li>
<li>Reformer: The Efficient Transformer (<a href="https://arxiv.org/abs/2001.04451">arxiv</a>)</li>
</ul>
<h2 id="support-license-and-copyright">Support, License and Copyright</h2>
<p>This software is distributed with the <strong>MIT</strong> license which pretty much means that
you can use it however you want and for whatever reason you want. All the
information regarding support, copyright and the license can be found in the
<a href="https://github.com/idiap/fast-transformers/blob/master/LICENSE">LICENSE</a> file
in the repository.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="transformers/" class="btn btn-neutral float-right" title="Transformers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/idiap/fast-transformers/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
      
        <span style="margin-left: 15px"><a href="transformers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>

<!--
MkDocs version : 1.1
Build Date UTC : 2020-07-01 13:29:04
-->
