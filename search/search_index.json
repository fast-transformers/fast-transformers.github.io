{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fast Transformers Transformers are very succsessfull models that achieve state of the art performance in many natural language tasks. However, it is very difficult to scale them to long sequences due to the quadratic scaling of self-attention. This library was developed for our research on fast attention for transformers. You can find a list of our papers below as well as related papers and papers that we have implemented. Quick-start The main interface of the library for using the implemented fast transformers is the builder interface . This allows for experimenting with different attention implentations with minimal code changes. For instance building a BERT-like transformer encoder is as simple as the following code: import torch from fast_transformers.builders import TransformerEncoderBuilder # Build a transformer encoder bert = TransformerEncoderBuilder.from_kwargs( n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, attention_type=\"full\", # change this to use another # attention implementation activation=\"gelu\" ).get() y = bert(torch.rand( 10, # batch_size 512, # sequence length 64*12 # features )) Installation The fast transformers library has the following dependencies: PyTorch C++ toolchain CUDA toolchain (if you want to compile for GPUs) For most machines installation should be as simple as: pip install --user pytorch-fast-transformers Research Ours To read about the theory behind some attention implementations in this library we encourage you to follow our research. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention ( arxiv , video ) Fast Transformers with Clustered Attention ( arxiv , blog ) If you found our research helpful or influential please consider citing @inproceedings{katharopoulos_et_al_2020, author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.}, title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, booktitle = {Proceedings of the International Conference on Machine Learning (ICML)}, year = {2020} } @article{vyas_et_al_2020, author={Vyas, A. and Katharopoulos, A. and Fleuret, F.}, title={Fast Transformers with Clustered Attention}, journal={arXiv preprint arXiv:2007.04825}, year={2020} } By others Efficient Attention: Attention with Linear Complexities ( arxiv ) Linformer: Self-Attention with Linear Complexity ( arxiv ) Reformer: The Efficient Transformer ( arxiv ) Support, License and Copyright This software is distributed with the MIT license which pretty much means that you can use it however you want and for whatever reason you want. All the information regarding support, copyright and the license can be found in the LICENSE file in the repository.","title":"Home"},{"location":"#fast-transformers","text":"Transformers are very succsessfull models that achieve state of the art performance in many natural language tasks. However, it is very difficult to scale them to long sequences due to the quadratic scaling of self-attention. This library was developed for our research on fast attention for transformers. You can find a list of our papers below as well as related papers and papers that we have implemented.","title":"Fast Transformers"},{"location":"#quick-start","text":"The main interface of the library for using the implemented fast transformers is the builder interface . This allows for experimenting with different attention implentations with minimal code changes. For instance building a BERT-like transformer encoder is as simple as the following code: import torch from fast_transformers.builders import TransformerEncoderBuilder # Build a transformer encoder bert = TransformerEncoderBuilder.from_kwargs( n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, attention_type=\"full\", # change this to use another # attention implementation activation=\"gelu\" ).get() y = bert(torch.rand( 10, # batch_size 512, # sequence length 64*12 # features ))","title":"Quick-start"},{"location":"#installation","text":"The fast transformers library has the following dependencies: PyTorch C++ toolchain CUDA toolchain (if you want to compile for GPUs) For most machines installation should be as simple as: pip install --user pytorch-fast-transformers","title":"Installation"},{"location":"#research","text":"","title":"Research"},{"location":"#ours","text":"To read about the theory behind some attention implementations in this library we encourage you to follow our research. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention ( arxiv , video ) Fast Transformers with Clustered Attention ( arxiv , blog ) If you found our research helpful or influential please consider citing @inproceedings{katharopoulos_et_al_2020, author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.}, title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, booktitle = {Proceedings of the International Conference on Machine Learning (ICML)}, year = {2020} } @article{vyas_et_al_2020, author={Vyas, A. and Katharopoulos, A. and Fleuret, F.}, title={Fast Transformers with Clustered Attention}, journal={arXiv preprint arXiv:2007.04825}, year={2020} }","title":"Ours"},{"location":"#by-others","text":"Efficient Attention: Attention with Linear Complexities ( arxiv ) Linformer: Self-Attention with Linear Complexity ( arxiv ) Reformer: The Efficient Transformer ( arxiv )","title":"By others"},{"location":"#support-license-and-copyright","text":"This software is distributed with the MIT license which pretty much means that you can use it however you want and for whatever reason you want. All the information regarding support, copyright and the license can be found in the LICENSE file in the repository.","title":"Support, License and Copyright"},{"location":"attention/","text":"Attention The attention module contains all the implementations of self-attention in the library. Details for each one are provided in the API docs but in this page of the documentation we will mention a few concepts that pertain all the implementations. Queries, keys, values Most self-attention implementations project the input queries, keys and values to multiple heads before computing the new values as a form of weighted average. Also this weighted average is again passed through a fully connected layer before returned as the output of the attention module. All those projections are handled by fast_transformers.attention.attention_layer.AttentionLayer which is described below. Note, that the AttentionLayer accepts an attention implementation as a first argument. This allows us to reuse the code that does the query, key, value and output projections and focus only on implementing efficient attention mechanisms. AttentionLayer fast_transformers.attention.attention_layer.AttentionLayer(attention, d_model, n_heads, d_keys=None, d_values=None) Arguments attention : Specific inner attention implementation that just computes a weighted average of values given a similarity of queries and keys. d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention d_keys : The dimensionality of the keys/queries (default: d_model/n_heads) d_values : The dimensionality of the values (default: d_model/n_heads) Masking The forward() method of all attention implementations accepts the following three masks, as objects that implement the BaseMask interface. attn_mask : This mask encodes the positions of the keys that each query is allowed to attend to. It is simply known as the attention mask. In PyTorch it is referred to as attn_mask or src_mask . query_lengths : This mask, usually a LengthMask , encodes the number of queries in each sample of the batch. key_lengths : Similar to the query_lengths mask, this mask encodes the number of keys and values in each sample of the batch. Shapes The transformer layers that use the attention modules are agnostic of the concept of attention heads. They call the attention with queries, keys and values of the following shape: Argument Shape queries (N, L, D) keys (N, S, D) values (N, S, M) In the table above, N denotes the batch size, L denotes the maximum number of queries in a sample, S denotes the maximum number of keys/values in a sample and D, M are the query/key dimensions and value dimensions respectively. The AttentionLayer , however, projects the arguments to multiple heads and calls the attention implementation with the following shapes, where H denotes the number of heads. Argument Shape queries (N, L, H, D) keys (N, S, H, D) values (N, S, H, M) Available Attentions The following is a list with the available attention implementations. Since this list is not automatically updated we suggest the reader to use the API Docs for an exhaustive list of attention implementations. FullAttention LinearAttention CausalLinearAttention ImprovedClusteredAttention ClusteredAttention ReformerAttention ConditionalFullAttention","title":"Attention"},{"location":"attention/#attention","text":"The attention module contains all the implementations of self-attention in the library. Details for each one are provided in the API docs but in this page of the documentation we will mention a few concepts that pertain all the implementations.","title":"Attention"},{"location":"attention/#queries-keys-values","text":"Most self-attention implementations project the input queries, keys and values to multiple heads before computing the new values as a form of weighted average. Also this weighted average is again passed through a fully connected layer before returned as the output of the attention module. All those projections are handled by fast_transformers.attention.attention_layer.AttentionLayer which is described below. Note, that the AttentionLayer accepts an attention implementation as a first argument. This allows us to reuse the code that does the query, key, value and output projections and focus only on implementing efficient attention mechanisms.","title":"Queries, keys, values"},{"location":"attention/#attentionlayer","text":"fast_transformers.attention.attention_layer.AttentionLayer(attention, d_model, n_heads, d_keys=None, d_values=None) Arguments attention : Specific inner attention implementation that just computes a weighted average of values given a similarity of queries and keys. d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention d_keys : The dimensionality of the keys/queries (default: d_model/n_heads) d_values : The dimensionality of the values (default: d_model/n_heads)","title":"AttentionLayer"},{"location":"attention/#masking","text":"The forward() method of all attention implementations accepts the following three masks, as objects that implement the BaseMask interface. attn_mask : This mask encodes the positions of the keys that each query is allowed to attend to. It is simply known as the attention mask. In PyTorch it is referred to as attn_mask or src_mask . query_lengths : This mask, usually a LengthMask , encodes the number of queries in each sample of the batch. key_lengths : Similar to the query_lengths mask, this mask encodes the number of keys and values in each sample of the batch.","title":"Masking"},{"location":"attention/#shapes","text":"The transformer layers that use the attention modules are agnostic of the concept of attention heads. They call the attention with queries, keys and values of the following shape: Argument Shape queries (N, L, D) keys (N, S, D) values (N, S, M) In the table above, N denotes the batch size, L denotes the maximum number of queries in a sample, S denotes the maximum number of keys/values in a sample and D, M are the query/key dimensions and value dimensions respectively. The AttentionLayer , however, projects the arguments to multiple heads and calls the attention implementation with the following shapes, where H denotes the number of heads. Argument Shape queries (N, L, H, D) keys (N, S, H, D) values (N, S, H, M)","title":"Shapes"},{"location":"attention/#available-attentions","text":"The following is a list with the available attention implementations. Since this list is not automatically updated we suggest the reader to use the API Docs for an exhaustive list of attention implementations. FullAttention LinearAttention CausalLinearAttention ImprovedClusteredAttention ClusteredAttention ReformerAttention ConditionalFullAttention","title":"Available Attentions"},{"location":"builders/","text":"Builders The builders module takes care of simplifying the construction of transformer networks. The following example showcases how simple it is to create a transformer encoder using the TransformerEncoderBuilder . import torch # Building without a builder from fast_transformers.transformers import TransformerEncoder, \\ TransformerEncoderLayer from fast_transformers.attention import AttentionLayer, FullAttention bert = TransformerEncoder( [ TransformerEncoderLayer( AttentionLayer(FullAttention(), 768, 12), 768, 12, activation=\"gelu\" ) for l in range(12) ], norm_layer=torch.nn.LayerNorm(768) ) # Building with a builder from fast_transformers.builders import TransformerEncoderBuilder bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"full\", n_layers=12, n_heads=12, feed_forward_dimensions=768*4, query_dimensions=768, activation=\"gelu\" ) Although it seems that the creation of a transformer is as simple with and without the builder, it becomes apparent that changing the creation logic with the builder is significantly easier. For instance, the attention_type can be read from a configuration file or from command line arguments. The rest of this page describes the API of the builders. Builder API The interface for all the builders is a simple method get() without any arguments that returns a PyTorch module that implements a transformer. All the parameters of the builders are simple python properties that can be set after the creation of the builder object. builder = ... # create a builder builder.parameter = value # set a parameter builder.other_parameter = other_value # and another parameter transformer = builder.get() # construct the transformer builder.parameter = changed_value # change a parameter other_transformer = builder.get() # construct another transformer The BaseBuilder provides helper static methods that make it simpler to set multiple builder arguments at once from configuration files or command line arguments. from_dictionary(dictionary, strict=True) Construct a builder and set all the parameters in the dictionary. If strict is set to True then throw a ValueError in case a dictionary key does not correspond to a builder parameter. from_kwargs(**kwargs) Construct a builder and set all the keyword arguments as builder parameters. from_namespace(args, strict=False) Construct a builder from an argument list returned by the python argparse module. If strict is set to True then throw a ValueError in case an argument does not correspond to a builder parameter. Transformer Builders There exist the following transformer builders for creating encoder and decoder architectures for inference and training: TransformerEncoderBuilder builds instances of TransformerEncoder TransformerDecoderBuilder builds instances of TransformerDecoder RecurrentEncoderBuilder builds instances of RecurrentTransformerEncoder RecurrentDecoderBuilder builds instances of RecurrentTransformerDecoder Attention Builders Attention builders simplify the construction of the various attention modules and allow for plugin-like extension mechanisms when creating new attention implementations. Their API is the same as the transformer builders, namely they accept attributes as parameters and then calling get(attention_type: str) constructs an nn.Module that implements an attention layer. from fast_transformers.builders import AttentionBuilder builder = AttentionBuilder.from_kwargs( attention_dropout=0.1, # used by softmax attention softmax_temp=1., # used by softmax attention feature_map=lambda x: (x>0).float() * x # used by linear ) softmax = builder.get(\"full\") linear = builder.get(\"linear\") The library provides the following attention builders that create the correspondingly named attention modules. AttentionBuilder RecurrentAttentionBuilder RecurrentCrossAttentionBuilder Attention composition The attention builders allow for attention composition through a simple convention of the attention_type parameter. Attention composition allows the creation of an attention layer that accepts one or more attention layers as a parameters. An example of this pattern is the ConditionalFullAttention that performs full softmax attention when the sequence length is small and delegates to another attention type when the sequence length becomes large. The following example code creates an attention layer that uses improved clustered attention for sequences larger than 512 elements and full softmax attention otherwise. builder = AttentionBuilder.from_kwargs( attention_dropout=0.1, # used by all softmax_temp=0.125, topk=32, # used by improved clustered clusters=256, bits=32, length_limit=512 # used by conditional attention ) attention = builder.get(\"conditional-full:improved-clustered\") Note Attention layers that are designed for composition cannot be used standalone. For instance conditional-full is not a valid attention type by itsself. Attention Registry The attention builders allow the dynamic registering of attention implementations through an attention registry . There are three registries, one for each available builder. You can find plenty of usage examples in the provided attention implementations (e.g. FullAttention ). This should only concern developers of new attention implementations and a simple example can be found in the custom attention layer section of the docs.","title":"Builders"},{"location":"builders/#builders","text":"The builders module takes care of simplifying the construction of transformer networks. The following example showcases how simple it is to create a transformer encoder using the TransformerEncoderBuilder . import torch # Building without a builder from fast_transformers.transformers import TransformerEncoder, \\ TransformerEncoderLayer from fast_transformers.attention import AttentionLayer, FullAttention bert = TransformerEncoder( [ TransformerEncoderLayer( AttentionLayer(FullAttention(), 768, 12), 768, 12, activation=\"gelu\" ) for l in range(12) ], norm_layer=torch.nn.LayerNorm(768) ) # Building with a builder from fast_transformers.builders import TransformerEncoderBuilder bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"full\", n_layers=12, n_heads=12, feed_forward_dimensions=768*4, query_dimensions=768, activation=\"gelu\" ) Although it seems that the creation of a transformer is as simple with and without the builder, it becomes apparent that changing the creation logic with the builder is significantly easier. For instance, the attention_type can be read from a configuration file or from command line arguments. The rest of this page describes the API of the builders.","title":"Builders"},{"location":"builders/#builder-api","text":"The interface for all the builders is a simple method get() without any arguments that returns a PyTorch module that implements a transformer. All the parameters of the builders are simple python properties that can be set after the creation of the builder object. builder = ... # create a builder builder.parameter = value # set a parameter builder.other_parameter = other_value # and another parameter transformer = builder.get() # construct the transformer builder.parameter = changed_value # change a parameter other_transformer = builder.get() # construct another transformer The BaseBuilder provides helper static methods that make it simpler to set multiple builder arguments at once from configuration files or command line arguments. from_dictionary(dictionary, strict=True) Construct a builder and set all the parameters in the dictionary. If strict is set to True then throw a ValueError in case a dictionary key does not correspond to a builder parameter. from_kwargs(**kwargs) Construct a builder and set all the keyword arguments as builder parameters. from_namespace(args, strict=False) Construct a builder from an argument list returned by the python argparse module. If strict is set to True then throw a ValueError in case an argument does not correspond to a builder parameter.","title":"Builder API"},{"location":"builders/#transformer-builders","text":"There exist the following transformer builders for creating encoder and decoder architectures for inference and training: TransformerEncoderBuilder builds instances of TransformerEncoder TransformerDecoderBuilder builds instances of TransformerDecoder RecurrentEncoderBuilder builds instances of RecurrentTransformerEncoder RecurrentDecoderBuilder builds instances of RecurrentTransformerDecoder","title":"Transformer Builders"},{"location":"builders/#attention-builders","text":"Attention builders simplify the construction of the various attention modules and allow for plugin-like extension mechanisms when creating new attention implementations. Their API is the same as the transformer builders, namely they accept attributes as parameters and then calling get(attention_type: str) constructs an nn.Module that implements an attention layer. from fast_transformers.builders import AttentionBuilder builder = AttentionBuilder.from_kwargs( attention_dropout=0.1, # used by softmax attention softmax_temp=1., # used by softmax attention feature_map=lambda x: (x>0).float() * x # used by linear ) softmax = builder.get(\"full\") linear = builder.get(\"linear\") The library provides the following attention builders that create the correspondingly named attention modules. AttentionBuilder RecurrentAttentionBuilder RecurrentCrossAttentionBuilder","title":"Attention Builders"},{"location":"builders/#attention-composition","text":"The attention builders allow for attention composition through a simple convention of the attention_type parameter. Attention composition allows the creation of an attention layer that accepts one or more attention layers as a parameters. An example of this pattern is the ConditionalFullAttention that performs full softmax attention when the sequence length is small and delegates to another attention type when the sequence length becomes large. The following example code creates an attention layer that uses improved clustered attention for sequences larger than 512 elements and full softmax attention otherwise. builder = AttentionBuilder.from_kwargs( attention_dropout=0.1, # used by all softmax_temp=0.125, topk=32, # used by improved clustered clusters=256, bits=32, length_limit=512 # used by conditional attention ) attention = builder.get(\"conditional-full:improved-clustered\") Note Attention layers that are designed for composition cannot be used standalone. For instance conditional-full is not a valid attention type by itsself.","title":"Attention composition"},{"location":"builders/#attention-registry","text":"The attention builders allow the dynamic registering of attention implementations through an attention registry . There are three registries, one for each available builder. You can find plenty of usage examples in the provided attention implementations (e.g. FullAttention ). This should only concern developers of new attention implementations and a simple example can be found in the custom attention layer section of the docs.","title":"Attention Registry"},{"location":"custom_attention_layer/","text":"Creating a custom attention layer In this page, we will go through the process of creating a custom attention module and integrating it with the library. We will implement a quadratic kernel attention instead of softmax attention. New Attention Our attention layer will follow closely the implementation of FullAttention . Let's start with the skeleton of our module. class QuadraticAttention(Module): def __init__(self, quadratic_temp=1.0, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps self.quadratic_temp = quadratic_temp def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # implement the logic of the layer here The queries, keys and values are already projected and split into multiple heads by the AttentionLayer . This means that we need only implement the attention part. class QuadraticAttention(Module): def __init__(self, quadratic_temp=1.0, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps self.quadratic_temp = quadratic_temp def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # compute the unnormalized attention QK = torch.einsum(\"nlhe,nshe->nhls\", queries, keys) # compute the dot products QK = torch.square(self.quadratic_temp * QK) # implement our custom attention twist QK = QK * attn_mask.float_matrix # use the attention mask as a multiplicative mask QK = QK * key_lengths.float_matrix[:, None, None] # also a multiplicative mask # normalize and compute the average A = QK / (QK.sum(dim=-1, keepdim=True) + self.eps) V = torch.einsum(\"nhls,nshd->nlhd\", A, values) return V.contiguous() Integrate with the Builder To add it as an option to the TransformerEncoderBuilder or the TransformerDecoderBuilder we have to register our new attention in the appropriate attention registry . The available registries are AttentionRegistry RecurrentAttentionRegistry RecurrentCrossAttentionRegistry Similar to FullAttention we will use AttentionRegistry because our implementation is not recurrent. The following snippet integrates our quadratic attention with the builders. from fast_transformers.attention_registry import AttentionRegistry, \\ Optional, Float # we also need these to add our new # parameter 'quadratic_temp' AttentionRegistry.register( \"square\", QuadraticAttention, # attention_type, class pair [ (\"quadratic_temp\", Optional(Float, 1.0)) # an optional parameter named # 'quadratic_temp' of type # float and with default # value 1.0 ] ) Afterwards we can use the builder to create transformers with our new attention layer. quadratic_bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"square\", # here we select our custom attention layer n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, activation=\"gelu\", quadratic_temp=5.0 # set the temperature for our quadratic layer )","title":"Custom Attention Layer"},{"location":"custom_attention_layer/#creating-a-custom-attention-layer","text":"In this page, we will go through the process of creating a custom attention module and integrating it with the library. We will implement a quadratic kernel attention instead of softmax attention.","title":"Creating a custom attention layer"},{"location":"custom_attention_layer/#new-attention","text":"Our attention layer will follow closely the implementation of FullAttention . Let's start with the skeleton of our module. class QuadraticAttention(Module): def __init__(self, quadratic_temp=1.0, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps self.quadratic_temp = quadratic_temp def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # implement the logic of the layer here The queries, keys and values are already projected and split into multiple heads by the AttentionLayer . This means that we need only implement the attention part. class QuadraticAttention(Module): def __init__(self, quadratic_temp=1.0, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps self.quadratic_temp = quadratic_temp def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # compute the unnormalized attention QK = torch.einsum(\"nlhe,nshe->nhls\", queries, keys) # compute the dot products QK = torch.square(self.quadratic_temp * QK) # implement our custom attention twist QK = QK * attn_mask.float_matrix # use the attention mask as a multiplicative mask QK = QK * key_lengths.float_matrix[:, None, None] # also a multiplicative mask # normalize and compute the average A = QK / (QK.sum(dim=-1, keepdim=True) + self.eps) V = torch.einsum(\"nhls,nshd->nlhd\", A, values) return V.contiguous()","title":"New Attention"},{"location":"custom_attention_layer/#integrate-with-the-builder","text":"To add it as an option to the TransformerEncoderBuilder or the TransformerDecoderBuilder we have to register our new attention in the appropriate attention registry . The available registries are AttentionRegistry RecurrentAttentionRegistry RecurrentCrossAttentionRegistry Similar to FullAttention we will use AttentionRegistry because our implementation is not recurrent. The following snippet integrates our quadratic attention with the builders. from fast_transformers.attention_registry import AttentionRegistry, \\ Optional, Float # we also need these to add our new # parameter 'quadratic_temp' AttentionRegistry.register( \"square\", QuadraticAttention, # attention_type, class pair [ (\"quadratic_temp\", Optional(Float, 1.0)) # an optional parameter named # 'quadratic_temp' of type # float and with default # value 1.0 ] ) Afterwards we can use the builder to create transformers with our new attention layer. quadratic_bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"square\", # here we select our custom attention layer n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, activation=\"gelu\", quadratic_temp=5.0 # set the temperature for our quadratic layer )","title":"Integrate with the Builder"},{"location":"masking/","text":"Masking In this library, both for convenience and efficiency, we define a BaseMask interface that all masks should implement. The BaseMask interface allows accessing a mask in the following ways: a bool tensor where True signifies what is kept a float tensor where minus infinity signifies what is to be masked a float tensor where zero signifies what is to be masked a length tensor where everything after a certain length is to be masked This interface allows us to use the same mask definition with various attention implementations without compromising in performance or requiring code changes. For instance, softmax masks are usually implemented with additive masks that contain -inf and linear attention masks are efficiently implemented with multiplicative masks that contain zeros. BaseMask Our API docs are quite thorough in explaining the BaseMask interface. Implementations We provide three implementations of the BaseMask interface FullMask , LengthMask and TriangularCausalMask . FullMask fast_transformers.masking.FullMask(mask=None, N=None, M=None, device='cpu') The FullMask is a simple wrapper over a pytorch boolean tensor. The arguments can be given both by keyword arguments and positional arguments. To imitate function overloading, the constructor checks the type of the first argument and if it is a tensor it treats it as the mask. otherwise it assumes that it was the N argument. Arguments mask : The mask as a PyTorch tensor. N : The rows of the all True mask to be created if the mask argument is not provided. M : The columns of the all True mask to be created if the mask argument is not provided. If N is given M defaults to N. device : The device to create the mask in (defaults to cpu) LengthMask fast_transformers.masking.LengthMask(lengths, max_len=None, device=None) The LengthMask is designed to be used for conveying different lengths of sequences. It can be accessed as an array of integers which may be beneficial for some attention implementations. Arguments lengths : The lengths as a PyTorch long tensor max_len : The maximum length for the mask (defaults to lengths.max()) device : The device to be used for creating the masks (defaults to lengths.device) TriangularCausalMask fast_transformers.masking.TriangularCausalMask(N, device=\"cpu\") Represents a square matrix with everything masked above the main diagonal. It is meant to be used for training autoregressive transformers. Arguments N : The size of the matrix device : The device to create the mask in (defaults to cpu)","title":"Masking"},{"location":"masking/#masking","text":"In this library, both for convenience and efficiency, we define a BaseMask interface that all masks should implement. The BaseMask interface allows accessing a mask in the following ways: a bool tensor where True signifies what is kept a float tensor where minus infinity signifies what is to be masked a float tensor where zero signifies what is to be masked a length tensor where everything after a certain length is to be masked This interface allows us to use the same mask definition with various attention implementations without compromising in performance or requiring code changes. For instance, softmax masks are usually implemented with additive masks that contain -inf and linear attention masks are efficiently implemented with multiplicative masks that contain zeros.","title":"Masking"},{"location":"masking/#basemask","text":"Our API docs are quite thorough in explaining the BaseMask interface.","title":"BaseMask"},{"location":"masking/#implementations","text":"We provide three implementations of the BaseMask interface FullMask , LengthMask and TriangularCausalMask .","title":"Implementations"},{"location":"masking/#fullmask","text":"fast_transformers.masking.FullMask(mask=None, N=None, M=None, device='cpu') The FullMask is a simple wrapper over a pytorch boolean tensor. The arguments can be given both by keyword arguments and positional arguments. To imitate function overloading, the constructor checks the type of the first argument and if it is a tensor it treats it as the mask. otherwise it assumes that it was the N argument. Arguments mask : The mask as a PyTorch tensor. N : The rows of the all True mask to be created if the mask argument is not provided. M : The columns of the all True mask to be created if the mask argument is not provided. If N is given M defaults to N. device : The device to create the mask in (defaults to cpu)","title":"FullMask"},{"location":"masking/#lengthmask","text":"fast_transformers.masking.LengthMask(lengths, max_len=None, device=None) The LengthMask is designed to be used for conveying different lengths of sequences. It can be accessed as an array of integers which may be beneficial for some attention implementations. Arguments lengths : The lengths as a PyTorch long tensor max_len : The maximum length for the mask (defaults to lengths.max()) device : The device to be used for creating the masks (defaults to lengths.device)","title":"LengthMask"},{"location":"masking/#triangularcausalmask","text":"fast_transformers.masking.TriangularCausalMask(N, device=\"cpu\") Represents a square matrix with everything masked above the main diagonal. It is meant to be used for training autoregressive transformers. Arguments N : The size of the matrix device : The device to create the mask in (defaults to cpu)","title":"TriangularCausalMask"},{"location":"recurrent_transformers/","text":"Recurrent Transformers The transformer layers implemented in the fast_transformers.transformers module are processing the entire sequence simultaneously. On the other hand, this module implements transfomers as recurrent networks. Namely as networks that process the sequence one element at a time while updating some state. The TransformerEncoder and TransformerEncoderLayer give way to RecurrentTransformerEncoder and RecurrentTransformerEncoderLayer and for the decoders RecurrentTransformerDecoder and RecurrentTransformerDecoderLayer respectively. Forward method RecurrentTransformerEncoder or RecurrentTransformerEncoderLayer forward(x, state=None) Arguments x : The input features of shape (N, E) where N is the batch size and E is d_model passed in the constructor. Note that x corresponds to a specific element in the sequence and not the entire sequence. state : The state is a python object that varies depending on the attention implementation RecurrentTransformerDecoder or RecurrentTransformerDecoderLayer forward(x, memory, memory_length_mask=None, state=None) x : The input features of shape (N, E) where N is the batch size and E is d_model passed in the constructor. Note that x corresponds to a specific element in the sequence and not the entire sequence. memory : A sequence of features (N, S, E) that the input will attend to. S is the sequence length and E is the same as for x. memory_length_mask : An implementation of a BaseMask that encodes how many elements each memory sequence in the batch consists of. state : The state is a python object that varies depending on the attention implementation Note The masks are different in the recurrent implementations than in their batch counterparts. Namely, recurrent encoders and decoders enforce a triangular causal mask on self attention. In addition, recurrent decoders enforce a full mask on cross attention. Available Attentions Not all attention formulations can be written in an autoregressive fashion as a recurrent model. In particular, since the sequence is passed to the transformer element by element we have the same result as passing a causal mask to normal transformers. The current list for recurrent attention implementations is: LinearAttention FullAttention Example The following example builds a random recurrent transformer encoder and applies its output as input 100 times. # for simplicity ignore all the classification # layers and the embedding layers from fast_transformers.builders import RecurrentEncoderBuilder model = RecurrentEncoderBuilder.from_kwargs( attention_type=\"linear\", n_layers=8, n_heads=12, feed_forward_dimensions=1536, query_dimensions=32, value_dimensions=32 ).get() x0 = torch.rand( 10, # batch size 12*32 # feature size ) state = None x = x0 for i in range(100): x, state = model(x, state=state)","title":"Recurrent Transformers"},{"location":"recurrent_transformers/#recurrent-transformers","text":"The transformer layers implemented in the fast_transformers.transformers module are processing the entire sequence simultaneously. On the other hand, this module implements transfomers as recurrent networks. Namely as networks that process the sequence one element at a time while updating some state. The TransformerEncoder and TransformerEncoderLayer give way to RecurrentTransformerEncoder and RecurrentTransformerEncoderLayer and for the decoders RecurrentTransformerDecoder and RecurrentTransformerDecoderLayer respectively.","title":"Recurrent Transformers"},{"location":"recurrent_transformers/#forward-method","text":"RecurrentTransformerEncoder or RecurrentTransformerEncoderLayer forward(x, state=None) Arguments x : The input features of shape (N, E) where N is the batch size and E is d_model passed in the constructor. Note that x corresponds to a specific element in the sequence and not the entire sequence. state : The state is a python object that varies depending on the attention implementation RecurrentTransformerDecoder or RecurrentTransformerDecoderLayer forward(x, memory, memory_length_mask=None, state=None) x : The input features of shape (N, E) where N is the batch size and E is d_model passed in the constructor. Note that x corresponds to a specific element in the sequence and not the entire sequence. memory : A sequence of features (N, S, E) that the input will attend to. S is the sequence length and E is the same as for x. memory_length_mask : An implementation of a BaseMask that encodes how many elements each memory sequence in the batch consists of. state : The state is a python object that varies depending on the attention implementation Note The masks are different in the recurrent implementations than in their batch counterparts. Namely, recurrent encoders and decoders enforce a triangular causal mask on self attention. In addition, recurrent decoders enforce a full mask on cross attention.","title":"Forward method"},{"location":"recurrent_transformers/#available-attentions","text":"Not all attention formulations can be written in an autoregressive fashion as a recurrent model. In particular, since the sequence is passed to the transformer element by element we have the same result as passing a causal mask to normal transformers. The current list for recurrent attention implementations is: LinearAttention FullAttention","title":"Available Attentions"},{"location":"recurrent_transformers/#example","text":"The following example builds a random recurrent transformer encoder and applies its output as input 100 times. # for simplicity ignore all the classification # layers and the embedding layers from fast_transformers.builders import RecurrentEncoderBuilder model = RecurrentEncoderBuilder.from_kwargs( attention_type=\"linear\", n_layers=8, n_heads=12, feed_forward_dimensions=1536, query_dimensions=32, value_dimensions=32 ).get() x0 = torch.rand( 10, # batch size 12*32 # feature size ) state = None x = x0 for i in range(100): x, state = model(x, state=state)","title":"Example"},{"location":"transformers/","text":"Transformers The fast_transformers.transformers module provides the TransformerEncoder and TransformerEncoderLayer classes, as well as their decoder counterparts, that implement a common transformer encoder/decoder similar to the PyTorch API. However, an important difference is that the TransformerEncoder does not create the TransformerEncoderLayer which allows for injecting a different implementation with minimal code changes. The encoder layer follows the same principle and does not create the attention layer but receives it as an argument which allows for using many different attention implementations with an otherwise identical model. We also provide recurrent transformer encoders and decoders which are meant to be given each input one at a time for autoregressive inference. Forward method TransformerEncoder or TransformerEncoderLayer forward(x, attn_mask=None, length_mask=None) Arguments x : The input features of shape (N, L, E) where N is the batch size, L is the sequence length (padded) and E is d_model passed in the constructor. attn_mask : An implementation of fast_transformers.masking.BaseMask that encodes where each element of x can attend to. length_mask : An implementation of fast_transformers.masking.BaseMask that encodes how many elements each sequence in the batch consists of. If the masks are not provided they are automatically created as an all ones mask for the attention mask and the size of the tensor for the length mask. TransformerDecoder or TransformerDecoderLayer forward(x, memory, x_mask=None, x_length_mask=None, memory_mask=None, memory_length_mask=None) Arguments x : The input features of shape (N, L, E) where N is the batch size, L is the sequence length (padded) and E should be the same as the d_model passed in the constructor. memory : The memory features of shape (N, L', E) where N is the batch size, L' is the memory's sequence length (padded) and E should be the same as the d_model . x_mask : An implementation of fast_transformers.masking.BaseMask that encodes where each element of x can attend to in x. Namely the self attention mask. x_length_mask : An implementation of a BaseMask that encodes how many elements each sequence in the batch consists of. memory_mask : An implementation of BaseMask that encodes where each element of x can attend to in the memory. Namely the cross attention mask. memory_length_mask : An implementation of a BaseMask that encodes how many elements each memory sequence in the batch consists of. Note Unlike the PyTorch transformer the dimensions of the input are ordered with the batch size first and the sequence second . TransformerEncoder fast_transformers.transformers.TransformerEncoder(layers, norm_layer=None) The TransformerEncoder is simply a container for transformer encoder layers that it receives as a list upon construction. Simply put it is a Sequential that is aware of masking and passes the masks to all the transformer encoder layers. Arguments layers : A list of TransformerEncoderLayer instances or other nn.Module instances that implement the same interface norm_layer : A normalization layer to be applied to the final output (default: None which means no normalization) TransformerEncoderLayer fast_transformers.transformers.TransformerEncoderLayer(attention, d_model, n_heads, d_ff=None, dropout=0.1, activation='relu') This transformer encoder layer implements the same encoder layer as PyTorch but is a bit more open for extension by receiving the attention implementation as a constructor argument. Arguments attention : The attention implementation to use given as a nn.Module d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention (Note: this parameter is unnecessary and will be removed in the near future) d_ff : The dimensionality of the intermediate features after the attention (default: d_model*4) dropout : The dropout rate to apply to the intermediate features (default: 0.1) activation : Choose which activation to use for the feed forward part of the layer from the set {'relu', 'gelu'} (default: relu) TransformerDecoder fast_transformers.transformers.TransformerDecoder(layers, norm_layer=None) The TransformerDecoder is simply a container for transformer decoder layers. These layers are passed as a list upon construction. Similar to the TransformerEncoder, it is a Sequential that is aware of masking and a second argument memory and properly forwards everything to the TransformerDecoderLayer instances. Arguments layers : A list of TransformerDecoderLayer instances or other nn.Module instances that implement the same interface norm_layer : A normalization layer to be applied to the final output (default: None which means no normalization) TransformerDecoderLayer fast_transformers.transformers.TransformerDecoderLayer(self_attention, cross_attention, d_model, d_ff=None, dropout=0.1, activation='relu') Similar to the encoder layer, this layer implements the decoder that PyTorch implements but can be used with any attention implementation because it receives the attention layers as constructor arguments. self_attention : The attention implementation to use for self attention given as a nn.Module cross_attention : The attention implementation to use for cross attention given as a nn.Module d_model : The input feature dimensionality d_ff : The dimensionality of the intermediate features after the attention (default: d_model*4) dropout : The dropout rate to apply to the intermediate features (default: 0.1) activation : Choose which activation to use for the feed forward part of the layer from the set {'relu', 'gelu'} (default: relu) Note The TransformerDecoderLayer accepts different attention layers for self attention and cross attention. This allows, for instance, for building transformers with linear self attention and softmax cross attention.","title":"Transformers"},{"location":"transformers/#transformers","text":"The fast_transformers.transformers module provides the TransformerEncoder and TransformerEncoderLayer classes, as well as their decoder counterparts, that implement a common transformer encoder/decoder similar to the PyTorch API. However, an important difference is that the TransformerEncoder does not create the TransformerEncoderLayer which allows for injecting a different implementation with minimal code changes. The encoder layer follows the same principle and does not create the attention layer but receives it as an argument which allows for using many different attention implementations with an otherwise identical model. We also provide recurrent transformer encoders and decoders which are meant to be given each input one at a time for autoregressive inference.","title":"Transformers"},{"location":"transformers/#forward-method","text":"TransformerEncoder or TransformerEncoderLayer forward(x, attn_mask=None, length_mask=None) Arguments x : The input features of shape (N, L, E) where N is the batch size, L is the sequence length (padded) and E is d_model passed in the constructor. attn_mask : An implementation of fast_transformers.masking.BaseMask that encodes where each element of x can attend to. length_mask : An implementation of fast_transformers.masking.BaseMask that encodes how many elements each sequence in the batch consists of. If the masks are not provided they are automatically created as an all ones mask for the attention mask and the size of the tensor for the length mask. TransformerDecoder or TransformerDecoderLayer forward(x, memory, x_mask=None, x_length_mask=None, memory_mask=None, memory_length_mask=None) Arguments x : The input features of shape (N, L, E) where N is the batch size, L is the sequence length (padded) and E should be the same as the d_model passed in the constructor. memory : The memory features of shape (N, L', E) where N is the batch size, L' is the memory's sequence length (padded) and E should be the same as the d_model . x_mask : An implementation of fast_transformers.masking.BaseMask that encodes where each element of x can attend to in x. Namely the self attention mask. x_length_mask : An implementation of a BaseMask that encodes how many elements each sequence in the batch consists of. memory_mask : An implementation of BaseMask that encodes where each element of x can attend to in the memory. Namely the cross attention mask. memory_length_mask : An implementation of a BaseMask that encodes how many elements each memory sequence in the batch consists of. Note Unlike the PyTorch transformer the dimensions of the input are ordered with the batch size first and the sequence second .","title":"Forward method"},{"location":"transformers/#transformerencoder","text":"fast_transformers.transformers.TransformerEncoder(layers, norm_layer=None) The TransformerEncoder is simply a container for transformer encoder layers that it receives as a list upon construction. Simply put it is a Sequential that is aware of masking and passes the masks to all the transformer encoder layers. Arguments layers : A list of TransformerEncoderLayer instances or other nn.Module instances that implement the same interface norm_layer : A normalization layer to be applied to the final output (default: None which means no normalization)","title":"TransformerEncoder"},{"location":"transformers/#transformerencoderlayer","text":"fast_transformers.transformers.TransformerEncoderLayer(attention, d_model, n_heads, d_ff=None, dropout=0.1, activation='relu') This transformer encoder layer implements the same encoder layer as PyTorch but is a bit more open for extension by receiving the attention implementation as a constructor argument. Arguments attention : The attention implementation to use given as a nn.Module d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention (Note: this parameter is unnecessary and will be removed in the near future) d_ff : The dimensionality of the intermediate features after the attention (default: d_model*4) dropout : The dropout rate to apply to the intermediate features (default: 0.1) activation : Choose which activation to use for the feed forward part of the layer from the set {'relu', 'gelu'} (default: relu)","title":"TransformerEncoderLayer"},{"location":"transformers/#transformerdecoder","text":"fast_transformers.transformers.TransformerDecoder(layers, norm_layer=None) The TransformerDecoder is simply a container for transformer decoder layers. These layers are passed as a list upon construction. Similar to the TransformerEncoder, it is a Sequential that is aware of masking and a second argument memory and properly forwards everything to the TransformerDecoderLayer instances. Arguments layers : A list of TransformerDecoderLayer instances or other nn.Module instances that implement the same interface norm_layer : A normalization layer to be applied to the final output (default: None which means no normalization)","title":"TransformerDecoder"},{"location":"transformers/#transformerdecoderlayer","text":"fast_transformers.transformers.TransformerDecoderLayer(self_attention, cross_attention, d_model, d_ff=None, dropout=0.1, activation='relu') Similar to the encoder layer, this layer implements the decoder that PyTorch implements but can be used with any attention implementation because it receives the attention layers as constructor arguments. self_attention : The attention implementation to use for self attention given as a nn.Module cross_attention : The attention implementation to use for cross attention given as a nn.Module d_model : The input feature dimensionality d_ff : The dimensionality of the intermediate features after the attention (default: d_model*4) dropout : The dropout rate to apply to the intermediate features (default: 0.1) activation : Choose which activation to use for the feed forward part of the layer from the set {'relu', 'gelu'} (default: relu) Note The TransformerDecoderLayer accepts different attention layers for self attention and cross attention. This allows, for instance, for building transformers with linear self attention and softmax cross attention.","title":"TransformerDecoderLayer"}]}