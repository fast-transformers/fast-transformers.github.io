{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Fast Transformers Transformers are very succsessfull models that achieve state of the art performance in many natural language tasks. However, it is very difficult to scale them to long sequences due to the quadratic scaling of self-attention. This library was developed for our research on fast attention for transformers. You can find a list of our papers below as well as related papers and papers that we have implemented. Quick-start The main interface of the library for using the implemented fast transformers is the builder interface . This allows for experimenting with different attention implentations with minimal code changes. For instance building a BERT-like transformer encoder is as simple as the following code: import torch from fast_transformers.builders import TransformerEncoderBuilder # Build a transformer encoder bert = TransformerEncoderBuilder.from_kwargs( n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, attention_type=\"full\", # change this to use another # attention implementation activation=\"gelu\" ).get() y = bert(torch.rand( 10, # batch_size 512, # sequence length 64*12 # features )) Installation The fast transformers library has the following dependencies: PyTorch C++ toolchain CUDA toolchain (if you want to compile for GPUs) For most machines installation should be as simple as: pip install --user pytorch-fast-transformers Research Ours To read about the theory behind some attention implementations in this library we encourage you to follow our research. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention ( arxiv , video ) Fast Transformers with Clustered Attention ( arxiv , blog ) If you found our research helpful or influential please consider citing @inproceedings{katharopoulos_et_al_2020, author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.}, title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, booktitle = {Proceedings of the International Conference on Machine Learning (ICML)}, year = {2020} } @article{vyas_et_al_2020, author={Vyas, A. and Katharopoulos, A. and Fleuret, F.}, title={Fast Transformers with Clustered Attention}, journal={arXiv preprint arXiv:2007.04825}, year={2020} } By others Efficient Attention: Attention with Linear Complexities ( arxiv ) Linformer: Self-Attention with Linear Complexity ( arxiv ) Reformer: The Efficient Transformer ( arxiv ) Support, License and Copyright This software is distributed with the MIT license which pretty much means that you can use it however you want and for whatever reason you want. All the information regarding support, copyright and the license can be found in the LICENSE file in the repository.","title":"Home"},{"location":"#fast-transformers","text":"Transformers are very succsessfull models that achieve state of the art performance in many natural language tasks. However, it is very difficult to scale them to long sequences due to the quadratic scaling of self-attention. This library was developed for our research on fast attention for transformers. You can find a list of our papers below as well as related papers and papers that we have implemented.","title":"Fast Transformers"},{"location":"#quick-start","text":"The main interface of the library for using the implemented fast transformers is the builder interface . This allows for experimenting with different attention implentations with minimal code changes. For instance building a BERT-like transformer encoder is as simple as the following code: import torch from fast_transformers.builders import TransformerEncoderBuilder # Build a transformer encoder bert = TransformerEncoderBuilder.from_kwargs( n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, attention_type=\"full\", # change this to use another # attention implementation activation=\"gelu\" ).get() y = bert(torch.rand( 10, # batch_size 512, # sequence length 64*12 # features ))","title":"Quick-start"},{"location":"#installation","text":"The fast transformers library has the following dependencies: PyTorch C++ toolchain CUDA toolchain (if you want to compile for GPUs) For most machines installation should be as simple as: pip install --user pytorch-fast-transformers","title":"Installation"},{"location":"#research","text":"","title":"Research"},{"location":"#ours","text":"To read about the theory behind some attention implementations in this library we encourage you to follow our research. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention ( arxiv , video ) Fast Transformers with Clustered Attention ( arxiv , blog ) If you found our research helpful or influential please consider citing @inproceedings{katharopoulos_et_al_2020, author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.}, title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, booktitle = {Proceedings of the International Conference on Machine Learning (ICML)}, year = {2020} } @article{vyas_et_al_2020, author={Vyas, A. and Katharopoulos, A. and Fleuret, F.}, title={Fast Transformers with Clustered Attention}, journal={arXiv preprint arXiv:2007.04825}, year={2020} }","title":"Ours"},{"location":"#by-others","text":"Efficient Attention: Attention with Linear Complexities ( arxiv ) Linformer: Self-Attention with Linear Complexity ( arxiv ) Reformer: The Efficient Transformer ( arxiv )","title":"By others"},{"location":"#support-license-and-copyright","text":"This software is distributed with the MIT license which pretty much means that you can use it however you want and for whatever reason you want. All the information regarding support, copyright and the license can be found in the LICENSE file in the repository.","title":"Support, License and Copyright"},{"location":"attention/","text":"Attention The attention module contains all the implementations of self-attention in the library. Details for each one are provided in the API docs but in this page of the documentation we will mention a few concepts that pertain all the implementations. Queries, keys, values Most self-attention implementations project the input queries, keys and values to multiple heads before computing the new values as a form of weighted average. Also this weighted average is again passed through a fully connected layer before returned as the output of the attention module. All those projections are handled by fast_transformers.attention.attention_layer.AttentionLayer which is described below. Note, that the AttentionLayer accepts an attention implementation as a first argument. This allows us to reuse the code that does the query, key, value and output projections and focus only on implementing efficient attention mechanisms. AttentionLayer fast_transformers.attention.attention_layer.AttentionLayer(attention, d_model, n_heads, d_keys=None, d_values=None) Arguments attention : Specific inner attention implementation that just computes a weighted average of values given a similarity of queries and keys. d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention d_keys : The dimensionality of the keys/queries (default: d_model/n_heads) d_values : The dimensionality of the values (default: d_model/n_heads) Masking The forward() method of all attention implementations accepts the following three masks, as objects that implement the BaseMask interface. attn_mask : This mask encodes the positions of the keys that each query is allowed to attend to. It is simply known as the attention mask. In PyTorch it is referred to as attn_mask or src_mask . query_lengths : This mask, usually a LengthMask , encodes the number of queries in each sample of the batch. key_lengths : Similar to the query_lengths mask, this mask encodes the number of keys and values in each sample of the batch. Shapes The transformer layers that use the attention modules are agnostic of the concept of attention heads. They call the attention with queries, keys and values of the following shape: Argument Shape queries (N, L, D) keys (N, S, D) values (N, S, M) In the table above, N denotes the batch size, L denotes the maximum number of queries in a sample, S denotes the maximum number of keys/values in a sample and D, M are the query/key dimensions and value dimensions respectively. The AttentionLayer , however, projects the arguments to multiple heads and calls the attention implementation with the following shapes, where H denotes the number of heads. Argument Shape queries (N, L, H, D) keys (N, S, H, D) values (N, S, H, M) Available Attentions The following is a list with the available attention implementations. Since this list is not automatically updated we suggest the reader to use the API Docs for an exhaustive list of attention implementations. FullAttention LinearAttention CausalLinearAttention ImprovedClusteredAttention ClusteredAttention ReformerAttention ConditionalFullAttention","title":"Attention"},{"location":"attention/#attention","text":"The attention module contains all the implementations of self-attention in the library. Details for each one are provided in the API docs but in this page of the documentation we will mention a few concepts that pertain all the implementations.","title":"Attention"},{"location":"attention/#queries-keys-values","text":"Most self-attention implementations project the input queries, keys and values to multiple heads before computing the new values as a form of weighted average. Also this weighted average is again passed through a fully connected layer before returned as the output of the attention module. All those projections are handled by fast_transformers.attention.attention_layer.AttentionLayer which is described below. Note, that the AttentionLayer accepts an attention implementation as a first argument. This allows us to reuse the code that does the query, key, value and output projections and focus only on implementing efficient attention mechanisms.","title":"Queries, keys, values"},{"location":"attention/#attentionlayer","text":"fast_transformers.attention.attention_layer.AttentionLayer(attention, d_model, n_heads, d_keys=None, d_values=None) Arguments attention : Specific inner attention implementation that just computes a weighted average of values given a similarity of queries and keys. d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention d_keys : The dimensionality of the keys/queries (default: d_model/n_heads) d_values : The dimensionality of the values (default: d_model/n_heads)","title":"AttentionLayer"},{"location":"attention/#masking","text":"The forward() method of all attention implementations accepts the following three masks, as objects that implement the BaseMask interface. attn_mask : This mask encodes the positions of the keys that each query is allowed to attend to. It is simply known as the attention mask. In PyTorch it is referred to as attn_mask or src_mask . query_lengths : This mask, usually a LengthMask , encodes the number of queries in each sample of the batch. key_lengths : Similar to the query_lengths mask, this mask encodes the number of keys and values in each sample of the batch.","title":"Masking"},{"location":"attention/#shapes","text":"The transformer layers that use the attention modules are agnostic of the concept of attention heads. They call the attention with queries, keys and values of the following shape: Argument Shape queries (N, L, D) keys (N, S, D) values (N, S, M) In the table above, N denotes the batch size, L denotes the maximum number of queries in a sample, S denotes the maximum number of keys/values in a sample and D, M are the query/key dimensions and value dimensions respectively. The AttentionLayer , however, projects the arguments to multiple heads and calls the attention implementation with the following shapes, where H denotes the number of heads. Argument Shape queries (N, L, H, D) keys (N, S, H, D) values (N, S, H, M)","title":"Shapes"},{"location":"attention/#available-attentions","text":"The following is a list with the available attention implementations. Since this list is not automatically updated we suggest the reader to use the API Docs for an exhaustive list of attention implementations. FullAttention LinearAttention CausalLinearAttention ImprovedClusteredAttention ClusteredAttention ReformerAttention ConditionalFullAttention","title":"Available Attentions"},{"location":"builders/","text":"Builders The builders module takes care of simplifying the construction of transformer networks. The following example showcases how simple it is to create a transformer encoder using the TransformerEncoderBuilder . import torch # Building without a builder from fast_transformers.transformers import TransformerEncoder, \\ TransformerEncoderLayer from fast_transformers.attention import AttentionLayer, FullAttention bert = TransformerEncoder( [ TransformerEncoderLayer( AttentionLayer(FullAttention(), 768, 12), 768, 12, activation=\"gelu\" ) for l in range(12) ], norm_layer=torch.nn.LayerNorm(768) ) # Building with a builder from fast_transformers.builders import TransformerEncoderBuilder bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"full\", n_layers=12, n_heads=12, feed_forward_dimensions=768*4, query_dimensions=768, activation=\"gelu\" ) Although it seems that the creation of a transformer is as simple with and without the builder, it becomes apparent that changing the creation logic with the builder is significantly easier. For instance, the attention_type can be read from a configuration file or from command line arguments. The rest of this page describes the API of the builders. Transformer Builder API The interface for all the builders is a simple method get() without any arguments that returns a PyTorch module that implements a transformer. All the parameters of the builders are simple python properties that can be set after the creation of the builder object. builder = ... # create a builder builder.parameter = value # set a parameter builder.other_parameter = other_value # and another parameter transformer = builder.get() # construct the transformer builder.parameter = changed_value # change a parameter other_transformer = builder.get() # construct another transformer The BaseTransformerBuilder provides helper static methods that make it simpler to set multiple builder arguments at once from configuration files or command line arguments. from_dictionary(dictionary, strict=True) Construct a builder and set all the parameters in the dictionary. If strict is set to True then throw a ValueError in case a dictionary key does not correspond to a builder parameter. from_kwargs(**kwargs) Construct a builder and set all the keyword arguments as builder parameters. from_namespace(args, strict=False) Construct a builder from an argument list returned by the python argparse module. If strict is set to True then throw a ValueError in case an argument does not correspond to a builder parameter. Available Builders TransformerEncoderBuilder constructs instances of TransformerEncoder RecurrentEncoderBuilder constructs instances of RecurrentTransformerEncoder","title":"Builders"},{"location":"builders/#builders","text":"The builders module takes care of simplifying the construction of transformer networks. The following example showcases how simple it is to create a transformer encoder using the TransformerEncoderBuilder . import torch # Building without a builder from fast_transformers.transformers import TransformerEncoder, \\ TransformerEncoderLayer from fast_transformers.attention import AttentionLayer, FullAttention bert = TransformerEncoder( [ TransformerEncoderLayer( AttentionLayer(FullAttention(), 768, 12), 768, 12, activation=\"gelu\" ) for l in range(12) ], norm_layer=torch.nn.LayerNorm(768) ) # Building with a builder from fast_transformers.builders import TransformerEncoderBuilder bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"full\", n_layers=12, n_heads=12, feed_forward_dimensions=768*4, query_dimensions=768, activation=\"gelu\" ) Although it seems that the creation of a transformer is as simple with and without the builder, it becomes apparent that changing the creation logic with the builder is significantly easier. For instance, the attention_type can be read from a configuration file or from command line arguments. The rest of this page describes the API of the builders.","title":"Builders"},{"location":"builders/#transformer-builder-api","text":"The interface for all the builders is a simple method get() without any arguments that returns a PyTorch module that implements a transformer. All the parameters of the builders are simple python properties that can be set after the creation of the builder object. builder = ... # create a builder builder.parameter = value # set a parameter builder.other_parameter = other_value # and another parameter transformer = builder.get() # construct the transformer builder.parameter = changed_value # change a parameter other_transformer = builder.get() # construct another transformer The BaseTransformerBuilder provides helper static methods that make it simpler to set multiple builder arguments at once from configuration files or command line arguments. from_dictionary(dictionary, strict=True) Construct a builder and set all the parameters in the dictionary. If strict is set to True then throw a ValueError in case a dictionary key does not correspond to a builder parameter. from_kwargs(**kwargs) Construct a builder and set all the keyword arguments as builder parameters. from_namespace(args, strict=False) Construct a builder from an argument list returned by the python argparse module. If strict is set to True then throw a ValueError in case an argument does not correspond to a builder parameter.","title":"Transformer Builder API"},{"location":"builders/#available-builders","text":"TransformerEncoderBuilder constructs instances of TransformerEncoder RecurrentEncoderBuilder constructs instances of RecurrentTransformerEncoder","title":"Available Builders"},{"location":"custom_attention_layer/","text":"Creating a custom attention layer In this page, we will go through the process of creating a custom attention module and integrating it with the library. We will implement a quadratic kernel attention instead of softmax attention. New Attention Our attention layer will follow closely the implementation of FullAttention . Let's start with the skeleton of our module. class QuadraticAttention(Module): def __init__(self, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # implement the logic of the layer here The queries, keys and values are already projected and split into multiple heads by the AttentionLayer . This means that we need only implement the attention part. class QuadraticAttention(Module): def __init__(self, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # compute the unnormalized attention QK = torch.einsum(\"nlhe,nshe->nhls\", queries, keys) # compute the dot products QK = torch.square(QK) # implement our custom attention twist QK = QK * attn_mask.float_matrix # use the attention mask as a multiplicative mask QK = QK * key_lengths.float_matrix[:, None, None] # also a multiplicative mask # normalize and compute the average A = QK / (QK.sum(dim=-1, keepdim=True) + self.eps) V = torch.einsum(\"nhls,nshd->nlhd\", A, values) return V.contiguous() Integrate with the Builder To add it as an option to the TransformerEncoderBuilder we have to edit two files. Firstly, we have to add it as an option to the AttentionBuilder.attention_type as follows: class AttentionBuilder(object): ... ... @attention_type.setter def attention_type(self, val): attentions = [\"full\", \"clustered\", \"improved-clustered\", \"improved-causal\", \"linear\", \"causal-linear\", \"reformer\", \"exact-topk\", \"square\"] # add the 'square' to the list if val not in attentions: raise ValueError((\"{!r} is not one of the available attention \" \"types {!r}\").format(val, attentions)) self._attention_type = val ... ... Secondly, we have to edit the TransformerEncoderBuilder to create the correct attention layer when the attention_type is set to 'square'. class TransformerEncoderBuilder(BaseTransformerBuilder): ... ... def _get_attention(self): attentions = { ... ... \"square\": QuadraticAttention } ... ... After those changes we can use the builder to create transformers with our new attention layer. quadratic_bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"square\", # here we select our custom attention layer n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, activation=\"gelu\" ) Future changes Editing two files to add a new attention implementation to a transformer builder is obviously suboptimal. We are in the process of changing this procedure with a more streamlined way of registering new attention layers to the transformer builders.","title":"Custom Attention Layer"},{"location":"custom_attention_layer/#creating-a-custom-attention-layer","text":"In this page, we will go through the process of creating a custom attention module and integrating it with the library. We will implement a quadratic kernel attention instead of softmax attention.","title":"Creating a custom attention layer"},{"location":"custom_attention_layer/#new-attention","text":"Our attention layer will follow closely the implementation of FullAttention . Let's start with the skeleton of our module. class QuadraticAttention(Module): def __init__(self, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # implement the logic of the layer here The queries, keys and values are already projected and split into multiple heads by the AttentionLayer . This means that we need only implement the attention part. class QuadraticAttention(Module): def __init__(self, eps=1e-6): super(QuadraticAttention, self).__init__() self.eps = eps def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths): # compute the unnormalized attention QK = torch.einsum(\"nlhe,nshe->nhls\", queries, keys) # compute the dot products QK = torch.square(QK) # implement our custom attention twist QK = QK * attn_mask.float_matrix # use the attention mask as a multiplicative mask QK = QK * key_lengths.float_matrix[:, None, None] # also a multiplicative mask # normalize and compute the average A = QK / (QK.sum(dim=-1, keepdim=True) + self.eps) V = torch.einsum(\"nhls,nshd->nlhd\", A, values) return V.contiguous()","title":"New Attention"},{"location":"custom_attention_layer/#integrate-with-the-builder","text":"To add it as an option to the TransformerEncoderBuilder we have to edit two files. Firstly, we have to add it as an option to the AttentionBuilder.attention_type as follows: class AttentionBuilder(object): ... ... @attention_type.setter def attention_type(self, val): attentions = [\"full\", \"clustered\", \"improved-clustered\", \"improved-causal\", \"linear\", \"causal-linear\", \"reformer\", \"exact-topk\", \"square\"] # add the 'square' to the list if val not in attentions: raise ValueError((\"{!r} is not one of the available attention \" \"types {!r}\").format(val, attentions)) self._attention_type = val ... ... Secondly, we have to edit the TransformerEncoderBuilder to create the correct attention layer when the attention_type is set to 'square'. class TransformerEncoderBuilder(BaseTransformerBuilder): ... ... def _get_attention(self): attentions = { ... ... \"square\": QuadraticAttention } ... ... After those changes we can use the builder to create transformers with our new attention layer. quadratic_bert = TransformerEncoderBuilder.from_kwargs( attention_type=\"square\", # here we select our custom attention layer n_layers=12, n_heads=12, query_dimensions=64, value_dimensions=64, feed_forward_dimensions=3072, activation=\"gelu\" )","title":"Integrate with the Builder"},{"location":"custom_attention_layer/#future-changes","text":"Editing two files to add a new attention implementation to a transformer builder is obviously suboptimal. We are in the process of changing this procedure with a more streamlined way of registering new attention layers to the transformer builders.","title":"Future changes"},{"location":"masking/","text":"Masking In this library, both for convenience and efficiency, we define a BaseMask interface that all masks should implement. The BaseMask interface allows accessing a mask in the following ways: a bool tensor where True signifies what is kept a float tensor where minus infinity signifies what is to be masked a float tensor where zero signifies what is to be masked a length tensor where everything after a certain length is to be masked This interface allows us to use the same mask definition with various attention implementations without compromising in performance or requiring code changes. For instance, softmax masks are usually implemented with additive masks that contain -inf and linear attention masks are efficiently implemented with multiplicative masks that contain zeros. BaseMask Our API docs are quite thorough in explaining the BaseMask interface. Implementations We provide three implementations of the BaseMask interface FullMask , LengthMask and TriangularCausalMask . FullMask fast_transformers.masking.FullMask(mask=None, N=None, M=None, device='cpu') The FullMask is a simple wrapper over a pytorch boolean tensor. The arguments can be given both by keyword arguments and positional arguments. To imitate function overloading, the constructor checks the type of the first argument and if it is a tensor it treats it as the mask. otherwise it assumes that it was the N argument. Arguments mask : The mask as a PyTorch tensor. N : The rows of the all True mask to be created if the mask argument is not provided. M : The columns of the all True mask to be created if the mask argument is not provided. If N is given M defaults to N. device : The device to create the mask in (defaults to cpu) LengthMask fast_transformers.masking.LengthMask(lengths, max_len=None, device=None) The LengthMask is designed to be used for conveying different lengths of sequences. It can be accessed as an array of integers which may be beneficial for some attention implementations. Arguments lengths : The lengths as a PyTorch long tensor max_len : The maximum length for the mask (defaults to lengths.max()) device : The device to be used for creating the masks (defaults to lengths.device) TriangularCausalMask fast_transformers.masking.TriangularCausalMask(N, device=\"cpu\") Represents a square matrix with everything masked above the main diagonal. It is meant to be used for training autoregressive transformers. Arguments N : The size of the matrix device : The device to create the mask in (defaults to cpu)","title":"Masking"},{"location":"masking/#masking","text":"In this library, both for convenience and efficiency, we define a BaseMask interface that all masks should implement. The BaseMask interface allows accessing a mask in the following ways: a bool tensor where True signifies what is kept a float tensor where minus infinity signifies what is to be masked a float tensor where zero signifies what is to be masked a length tensor where everything after a certain length is to be masked This interface allows us to use the same mask definition with various attention implementations without compromising in performance or requiring code changes. For instance, softmax masks are usually implemented with additive masks that contain -inf and linear attention masks are efficiently implemented with multiplicative masks that contain zeros.","title":"Masking"},{"location":"masking/#basemask","text":"Our API docs are quite thorough in explaining the BaseMask interface.","title":"BaseMask"},{"location":"masking/#implementations","text":"We provide three implementations of the BaseMask interface FullMask , LengthMask and TriangularCausalMask .","title":"Implementations"},{"location":"masking/#fullmask","text":"fast_transformers.masking.FullMask(mask=None, N=None, M=None, device='cpu') The FullMask is a simple wrapper over a pytorch boolean tensor. The arguments can be given both by keyword arguments and positional arguments. To imitate function overloading, the constructor checks the type of the first argument and if it is a tensor it treats it as the mask. otherwise it assumes that it was the N argument. Arguments mask : The mask as a PyTorch tensor. N : The rows of the all True mask to be created if the mask argument is not provided. M : The columns of the all True mask to be created if the mask argument is not provided. If N is given M defaults to N. device : The device to create the mask in (defaults to cpu)","title":"FullMask"},{"location":"masking/#lengthmask","text":"fast_transformers.masking.LengthMask(lengths, max_len=None, device=None) The LengthMask is designed to be used for conveying different lengths of sequences. It can be accessed as an array of integers which may be beneficial for some attention implementations. Arguments lengths : The lengths as a PyTorch long tensor max_len : The maximum length for the mask (defaults to lengths.max()) device : The device to be used for creating the masks (defaults to lengths.device)","title":"LengthMask"},{"location":"masking/#triangularcausalmask","text":"fast_transformers.masking.TriangularCausalMask(N, device=\"cpu\") Represents a square matrix with everything masked above the main diagonal. It is meant to be used for training autoregressive transformers. Arguments N : The size of the matrix device : The device to create the mask in (defaults to cpu)","title":"TriangularCausalMask"},{"location":"recurrent_transformers/","text":"Recurrent Transformers The transformer layers implemented in the fast_transformers.transformers module are processing the entire sequence simultaneously. On the other hand, this module implements transfomers as recurrent networks. Namely as networks that process the sequence one element at a time while updating some memory. Since this API is subject to change we will briefly go over the current differences in comparison to the normal transformers API. Forward method The TransformerEncoder and TransformerEncoderLayer give way to RecurrentTransformerEncoder and RecurrentTransformerEncoderLayer and the forward() method changes as follows: forward(x, memory=None) Arguments x : The input features of shape (N, E) where N is the batch size and E is d_model passed in the constructor. Note that x corresponds to a specific element in the sequence and not the entire sequence. memory : The memory is a python object that varies depending on the attention implementation Available Attentions Not all attention formulations can be written in an autoregressive fashion as a recurrent model. In particular, since the sequence is passed to the transformer element by element we have the same result as passing a causal mask to normal transformers. The current list for recurrent attention implementations is: LinearAttention FullAttention Builder Building a RecurrentTransformerEncoder is very similar to building a TransformerEncoder . We simply provide a different builder named RecurrentEncoderBuilder that constructs RecurrentTransformerEncoder models. Example The following example builds a random recurrent transformer encoder and applies its output as input 100 times. # for simplicity ignore all the classification # layers and the embedding layers from fast_transformers.builders import RecurrentEncoderBuilder model = RecurrentEncoderBuilder.from_kwargs( attention_type=\"linear\", n_layers=8, n_heads=12, feed_forward_dimensions=1536, query_dimensions=32, value_dimensions=32 ).get() x0 = torch.rand( 10, # batch size 12*32 # feature size ) memory = None x = x0 for i in range(100): x, memory = model(x, memory=memory)","title":"Recurrent Transformers"},{"location":"recurrent_transformers/#recurrent-transformers","text":"The transformer layers implemented in the fast_transformers.transformers module are processing the entire sequence simultaneously. On the other hand, this module implements transfomers as recurrent networks. Namely as networks that process the sequence one element at a time while updating some memory. Since this API is subject to change we will briefly go over the current differences in comparison to the normal transformers API.","title":"Recurrent Transformers"},{"location":"recurrent_transformers/#forward-method","text":"The TransformerEncoder and TransformerEncoderLayer give way to RecurrentTransformerEncoder and RecurrentTransformerEncoderLayer and the forward() method changes as follows: forward(x, memory=None) Arguments x : The input features of shape (N, E) where N is the batch size and E is d_model passed in the constructor. Note that x corresponds to a specific element in the sequence and not the entire sequence. memory : The memory is a python object that varies depending on the attention implementation","title":"Forward method"},{"location":"recurrent_transformers/#available-attentions","text":"Not all attention formulations can be written in an autoregressive fashion as a recurrent model. In particular, since the sequence is passed to the transformer element by element we have the same result as passing a causal mask to normal transformers. The current list for recurrent attention implementations is: LinearAttention FullAttention","title":"Available Attentions"},{"location":"recurrent_transformers/#builder","text":"Building a RecurrentTransformerEncoder is very similar to building a TransformerEncoder . We simply provide a different builder named RecurrentEncoderBuilder that constructs RecurrentTransformerEncoder models.","title":"Builder"},{"location":"recurrent_transformers/#example","text":"The following example builds a random recurrent transformer encoder and applies its output as input 100 times. # for simplicity ignore all the classification # layers and the embedding layers from fast_transformers.builders import RecurrentEncoderBuilder model = RecurrentEncoderBuilder.from_kwargs( attention_type=\"linear\", n_layers=8, n_heads=12, feed_forward_dimensions=1536, query_dimensions=32, value_dimensions=32 ).get() x0 = torch.rand( 10, # batch size 12*32 # feature size ) memory = None x = x0 for i in range(100): x, memory = model(x, memory=memory)","title":"Example"},{"location":"transformers/","text":"Transformers The fast_transformers.transformers module provides the TransformerEncoder and TransformerEncoderLayer classes that implement a common transformer encoder similar to the PyTorch API. However, an important difference is that the TransformerEncoder does not create the TransformerEncoderLayer which allows for injecting a different implementation with minimal code changes. The encoder layer follows the same principle and does not create the attention layer but receives it as an argument which allows for using many different attention implementations with an otherwise identical model. We also provide recurrent transformer encoders which are meant to be given each input one at a time for autoregressive inference. Forward method Both the transformer encoder and the transformer encoder layer accept the same parameters for the forward() method. forward(x, attn_mask=None, length_mask=None) Arguments x : The input features of shape (N, L, E) where N is the batch size, L is the sequence length (padded) and E is d_model passed in the constructor. attn_mask : An implementation of fast_transformers.masking.BaseMask that encodes where each element of x can attend to. length_mask : An implementation of fast_transformers.masking.BaseMask that encodes how many elements each sequence in the batch consists of. If the masks are not provided they are automatically created as an all ones mask for the attention mask and the size of the tensor for the length mask. Note Unlike the PyTorch transformer the dimensions of the input are ordered with the batch size first and the sequence second . TransformerEncoder fast_transformers.transformers.TransformerEncoder(layers, norm_layer=None) The TransformerEncoder is simply a container for transformer encoder layers that it receives as a list upon construction. Simply put it is a Sequential that is aware of masking and passes the masks to all the transformer encoder layers. Arguments layers : A list of TransformerEncoderLayer instances or other nn.Module instances that implement the same interface norm_layer : A normalization layer to be applied to the final output (default: None which means no normalization) TransformerEncoderLayer fast_transformers.transformers.TransformerEncoderLayer(attention, d_model, n_heads, d_ff=None, dropout=0.1, activation='relu') This transformer encoder layer implements the same encoder layer as PyTorch but is a bit more open for extension by receiving the attention implementation as a constructor argument. Arguments attention : The attention implementation to use given as a nn.Module d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention d_ff : The dimensionality of the intermediate features after the attention (default: d_model*4) dropout : The dropout rate to apply to the intermediate features (default: 0.1) activation : Choose which activation to use for the feed forward part of the layer from the set {'relu', 'gelu'} (default: relu)","title":"Transformers"},{"location":"transformers/#transformers","text":"The fast_transformers.transformers module provides the TransformerEncoder and TransformerEncoderLayer classes that implement a common transformer encoder similar to the PyTorch API. However, an important difference is that the TransformerEncoder does not create the TransformerEncoderLayer which allows for injecting a different implementation with minimal code changes. The encoder layer follows the same principle and does not create the attention layer but receives it as an argument which allows for using many different attention implementations with an otherwise identical model. We also provide recurrent transformer encoders which are meant to be given each input one at a time for autoregressive inference.","title":"Transformers"},{"location":"transformers/#forward-method","text":"Both the transformer encoder and the transformer encoder layer accept the same parameters for the forward() method. forward(x, attn_mask=None, length_mask=None) Arguments x : The input features of shape (N, L, E) where N is the batch size, L is the sequence length (padded) and E is d_model passed in the constructor. attn_mask : An implementation of fast_transformers.masking.BaseMask that encodes where each element of x can attend to. length_mask : An implementation of fast_transformers.masking.BaseMask that encodes how many elements each sequence in the batch consists of. If the masks are not provided they are automatically created as an all ones mask for the attention mask and the size of the tensor for the length mask. Note Unlike the PyTorch transformer the dimensions of the input are ordered with the batch size first and the sequence second .","title":"Forward method"},{"location":"transformers/#transformerencoder","text":"fast_transformers.transformers.TransformerEncoder(layers, norm_layer=None) The TransformerEncoder is simply a container for transformer encoder layers that it receives as a list upon construction. Simply put it is a Sequential that is aware of masking and passes the masks to all the transformer encoder layers. Arguments layers : A list of TransformerEncoderLayer instances or other nn.Module instances that implement the same interface norm_layer : A normalization layer to be applied to the final output (default: None which means no normalization)","title":"TransformerEncoder"},{"location":"transformers/#transformerencoderlayer","text":"fast_transformers.transformers.TransformerEncoderLayer(attention, d_model, n_heads, d_ff=None, dropout=0.1, activation='relu') This transformer encoder layer implements the same encoder layer as PyTorch but is a bit more open for extension by receiving the attention implementation as a constructor argument. Arguments attention : The attention implementation to use given as a nn.Module d_model : The input feature dimensionality n_heads : The number of heads for the multi head attention d_ff : The dimensionality of the intermediate features after the attention (default: d_model*4) dropout : The dropout rate to apply to the intermediate features (default: 0.1) activation : Choose which activation to use for the feed forward part of the layer from the set {'relu', 'gelu'} (default: relu)","title":"TransformerEncoderLayer"}]}