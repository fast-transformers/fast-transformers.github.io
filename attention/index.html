<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Attention - Fast Transformers for PyTorch</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Attention";
    var mkdocs_page_input_path = "attention.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Fast Transformers for PyTorch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../transformers/">Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../masking/">Masking</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Attention</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#queries-keys-values">Queries, keys, values</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#attentionlayer">AttentionLayer</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#masking">Masking</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#shapes">Shapes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#available-attentions">Available Attentions</a>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../feature_maps/">Feature Maps</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../builders/">Builders</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../custom_attention_layer/">Custom Attention Layer</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../recurrent_transformers/">Recurrent Transformers</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../events/">Events</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../tips_and_tricks/">Tips and Tricks</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="" href="/api_docs/fast_transformers/">API Docs</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Fast Transformers for PyTorch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Attention</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/idiap/fast-transformers/edit/master/docs/attention.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="attention">Attention</h1>
<p>The <a href="/api_docs/fast_transformers/attention/">attention</a> module contains all the implementations of self-attention in
the library. Details for each one are provided in the <a href="/api_docs/fast_transformers/attention/">API docs</a> but in this
page of the documentation we will mention a few concepts that pertain all the
implementations.</p>
<h2 id="queries-keys-values">Queries, keys, values</h2>
<p>Most self-attention implementations project the input queries, keys and values
to multiple heads before computing the new values as a form of weighted average.
Also this weighted average is again passed through a fully connected layer
before returned as the output of the attention module.</p>
<p>All those projections are handled by
<a href="/api_docs/fast_transformers/attention/attention_layer.html">fast_transformers.attention.attention_layer.AttentionLayer</a> which is
described below.</p>
<p>Note, that the AttentionLayer accepts an attention implementation as a first
argument. This allows us to reuse the code that does the query, key, value and
output projections and focus only on implementing efficient attention
mechanisms.</p>
<h3 id="attentionlayer">AttentionLayer</h3>
<pre><code>fast_transformers.attention.attention_layer.AttentionLayer(attention, d_model, n_heads, d_keys=None, d_values=None)
</code></pre>

<p><strong>Arguments</strong></p>
<ul>
<li><strong>attention</strong>: Specific inner attention implementation that just computes a
  weighted average of values given a similarity of queries and keys.</li>
<li><strong>d_model</strong>: The input feature dimensionality</li>
<li><strong>n_heads</strong>: The number of heads for the multi head attention</li>
<li><strong>d_keys</strong>: The dimensionality of the keys/queries
  (default: d_model/n_heads)</li>
<li><strong>d_values</strong>: The dimensionality of the values (default: d_model/n_heads)</li>
</ul>
<h2 id="masking">Masking</h2>
<p>The <code>forward()</code> method of all attention implementations accepts the following
three masks, as objects that implement the <a href="../masking/">BaseMask</a> interface.</p>
<ul>
<li><strong>attn_mask</strong>: This mask encodes the positions of the keys that each query is
  allowed to attend to. It is simply known as the attention mask. In PyTorch it
  is referred to as <code>attn_mask</code> or <code>src_mask</code>.</li>
<li><strong>query_lengths</strong>: This mask, usually a <a href="../masking/#lengthmask">LengthMask</a>,
  encodes the number of queries in each sample of the batch.</li>
<li><strong>key_lengths</strong>: Similar to the <code>query_lengths</code> mask, this mask encodes the
  number of keys and values in each sample of the batch.</li>
</ul>
<h2 id="shapes">Shapes</h2>
<p>The <a href="../transformers/">transformer layers</a> that use the attention modules are
agnostic of the concept of attention heads. They call the attention with
queries, keys and values of the following shape:</p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>queries</td>
<td>(N, L, D)</td>
</tr>
<tr>
<td>keys</td>
<td>(N, S, D)</td>
</tr>
<tr>
<td>values</td>
<td>(N, S, M)</td>
</tr>
</tbody>
</table>
<p>In the table above, N denotes the batch size, L denotes the maximum number of
queries in a sample, S denotes the maximum number of keys/values in a sample
and D, M are the query/key dimensions and value dimensions respectively.</p>
<p>The <a href="/api_docs/fast_transformers/attention/attention_layer.html">AttentionLayer</a>, however, projects the arguments to multiple heads and
calls the attention implementation with the following shapes, where H denotes
the number of heads.</p>
<table>
<thead>
<tr>
<th>Argument</th>
<th>Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>queries</td>
<td>(N, L, H, D)</td>
</tr>
<tr>
<td>keys</td>
<td>(N, S, H, D)</td>
</tr>
<tr>
<td>values</td>
<td>(N, S, H, M)</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<h2 id="available-attentions">Available Attentions</h2>
<p>The following is a list with the available attention implementations. Since
this list is not automatically updated we suggest the reader to use the <a href="/api_docs/fast_transformers/attention/">API
Docs</a> for an exhaustive list of attention implementations.</p>
<ul>
<li><a href="/api_docs/fast_transformers/attention/full_attention.html">FullAttention</a></li>
<li><a href="/api_docs/fast_transformers/attention/linear_attention.html">LinearAttention</a></li>
<li><a href="/api_docs/fast_transformers/attention/causal_linear_attention.html">CausalLinearAttention</a></li>
<li><a href="/api_docs/fast_transformers/attention/improved_clustered_attention.html">ImprovedClusteredAttention</a></li>
<li><a href="/api_docs/fast_transformers/attention/clustered_attention.html">ClusteredAttention</a></li>
<li><a href="/api_docs/fast_transformers/attention/reformer_attention.html">ReformerAttention</a></li>
<li><a href="/api_docs/fast_transformers/attention/local_attention.html">LocalAttention</a></li>
<li><a href="/api_docs/fast_transformers/attention/conditional_full_attention.html">ConditionalFullAttention</a></li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../feature_maps/" class="btn btn-neutral float-right" title="Feature Maps">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../masking/" class="btn btn-neutral" title="Masking"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/idiap/fast-transformers/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../masking/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../feature_maps/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
